---
title: "Analysing Standard Progressive Matrics (SPM-LS) with Bayesian Item Response Models"
shorttitle: "Bayesian Item Response Models"
author: 
  - name: Paul-Christian Bürkner
    affiliation: "1"
    corresponding: yes
    email: paul.buerkner@gmail.com
    address: Department of Computer Science, Aalto University, Konemiehentie 2, 02150 Espoo, Finland
affiliation:
  - id: 1
    institution: Department of Computer Science, Aalto University, Finland
abstract: |
  Raven’s Standard Progressive Matrices (SPM) test and related matrix-based
  tests are widely applied measures of cognitive ability. Using Bayesian Item
  Response Theory (IRT) models, I reanalyse data of an SPM short form proposed
  by Myszkowski & Storme (2018). Results indicate that a 3-parameter logistic
  (3PL) model is sufficient to describe participants dichotomous responses
  (correct vs. incorrect) while persons' ability parameters are quite robust
  across IRT models of varying complexity. These conclusions are in line with
  the original results of Myszkowski & Storme (2018). Using Bayesian as opposed
  to frequentist IRT models offered advantages in the estimation of more complex
  (i.e., 3-4PL) IRT models and provided more sensible and robust uncertainty
  estimates.
keywords: Standard Progressive Matrics, Item Response Theory, Bayesian Statistics, brms, Stan, R
lang: english
class: doc
lineno: yes
figsintext: true
# numbersections: false
encoding: UTF-8
bibliography:
  - SPM_IRT_paper.bib
output:
  papaja::apa6_pdf:
    highlight: default
header-includes:
   - \usepackage{mathtools}
   - \usepackage[utf8]{inputenc}
   - \usepackage[T1]{fontenc}
   - \usepackage{textcomp}
   - \usepackage{graphicx,pdflscape}
   - \usepackage{geometry}
   - \usepackage{amsmath}
   - \usepackage{float}
   - \usepackage{supertabular}
   - \usepackage{booktabs,caption,fixltx2e}
   - \usepackage[flushleft]{threeparttable}
   - \usepackage{natbib}
   - \usepackage{tcolorbox}
   - \usepackage{paralist}
   - \usepackage{multicol}
   - \newcommand\numberthis{\addtocounter{equation}{1}\tag{\theequation}}
editor_options: 
  chunk_output_type: console
---

```{r setup, message = FALSE, warning = FALSE, results = "hide", cache = FALSE}
library(knitr)
library(kableExtra)
library(papaja)
library(tidyverse)
library(patchwork)
library(brms)
library(mirt)

# set ggplot theme
theme_set(bayesplot::theme_default())

# set rstan options
rstan::rstan_options(auto_write = TRUE)
options(mc.cores = max(1, parallel::detectCores() - 1))

# enables / disables caching for all chunks of code
knitr::opts_chunk$set(
  cache = TRUE,
  warning = FALSE,
  message = FALSE,
  echo = FALSE
)
options(knitr.kable.NA = '')

# how to use papaja ? https://crsh.github.io/papaja_man/introduction.html
```

# Introduction

Raven’s Standard Progressive Matrices (SPM) test [@raven1941] and related
matrix-based tests are widely applied measures of cognitive ability [e.g.,
@jensen1988; @pind2003]. Due to their non-verbal content, which reduces biases
due to language and cultural differenes, they are considered one of the purest
measures of fluid intelligence [@myszkowski2018]. However, a disadvantage of the
original SPM is that its administration takes considerable time as 60 items have
to be answered and time limits are either very loose or not imposed at all
[e.g., @pind2003]. Thus, using it as part of a bigger procedure involving the
administration of multiple tests and/or experiments may be problematic. This is
not only due to direct time restrictions but also because participants'
motivation and concentration tends to decline over the course of the complete
procedure potentially leading to less valid measurements [e.g., @ackerman2009].

Recently, @myszkowski2018 have proposed a short version of the original SPM
test, called SPM-LS, comprising only the last block of 12 most complex SPM items
and evaluated its statistical properties using methods of Item Response Theory
(IRT). IRT is widely applied in the human sciences to model persons’ responses
on a set of items measuring one or more latent constructs [for a comprehensive
introduction see @lord2012; @embretson2013;
@vanderlinden2013]. Due to its flexibility compared to Classical Test Theory
(CTT), IRT provides the formal statistical basis for most modern psychological
measurement. The best known IRT models are likely those for binary responses,
which predict the probability of a correct answer depending on the item's
difficulty and potentially other item properties as well as the participant's
latent abilities. As responses on SPM items can be categorized as either right
or wrong, we will focus on these binary models in the present paper [although
other models for this data are possible as well; see @myszkowski2018].
@myszkowski2018, whose data I seek to reanalyse, used frequenstist IRT models
for inference. In this paper, I will apply Bayesian IRT models instead and
investigate potential differences to the original results. In doing so, I hope
to improve our unstanding of the robustness of the inference obtainable from the
SPM-LS test.

In Bayesian statistics applied to IRT, we aim to estimate the posterior
distribution $p(\theta, \xi | y)$ of the person and item parameters ($\theta$
and $\xi$, respectively) given the data $y$. We may be either interested in the
posterior distribution directly, or in quantities that can be computed on its
basis. The posterior distribution for an IRT model is defined as
\begin{equation}
p(\theta, \xi | y) = \frac{p(y | \theta, \xi) \, p(\theta, \xi)}{p(y)}.
\end{equation}
In the above equation $p(y | \theta, \xi)$ is the likelihood, $p(\theta, \xi)$
is the prior distribution and $p(y)$ is the marginal likelihood. The likelihood
$p(y | \theta, \xi)$ is the distribution of the data given the parameters and
thus relates the the data to the parameters. The prior distribution $p(\theta,
\xi)$ describes the uncertainty in the person and item parameters before having
seen the data. It thus allows to explicitely incorporate prior knowledge into
the model. In practice, we will factorize the joint prior $p(\theta, \xi)$ into
the product of $p(\theta)$ and $p(\xi)$ so that we can specify priors on person
and items parameters independently. We will detail likelihoods and priors for
Bayesian IRT models in the next section. The marginal likelihood $p(y)$ serves
as a normalizing constant so that the posterior is an actual probability
distribution. Except in the context of specific methods (i.e., Bayes factors),
$p(y)$ is rarely of direct interest.

In frequentist statistics, parameter estimates are usually obtained by finding
those parameter values that maximise the likelihood. In contrast, Bayesian
statistics estimate the full (joint) posterior distribution of the parameters.
This is not only fully consistent with probability theory, but also much more
informative than a single point estimate (and an approximate measure of
uncertainty commonly known as 'standard error'). 

Obtaining the posterior distribution analytically is only possible in certain
cases of carefully chosen combinations of prior and likelihood, which may
considerably limit modelling flexibilty but yield a computational advantage.
However, with the increased power of today's computers, Markov-Chain Monte-Carlo
(MCMC) sampling methods constitute a powerful and feasible alternative to
obtaining posterior distributions for complex models in which the majority of
modeling decisions is made based on theoretical and not computational grounds.
Despite all the computing power, these sampling algorithms are computationally
very intensive and thus fitting models using full Bayesian inference is usually
much slower than in point estimation techniques. However, advantages of Bayesian
inference -- such as greater modeling flexibility, prior distributions, and more
informative results -- are often worth the increased computational cost
[@gelman2013]. Whether Bayesian statistics offers relevant advantages for
the IRT modelling of the SPM-LS data will be investigated in this paper.

# Bayesian IRT Models

In this section, I will introduce a set of Bayesian IRT models for binary
data and unidimensional person traits. Suppose that for each pserson $j$ ($j = 1,
\ldots, J$) and item $i$ ($i = 1, \ldots, I$), we have observed a binary response
$y_{ji}$ which is coded as $1$ for a correct answer and $0$ otherwise.

## Model Likelihood

With our binary IRT models, we aim to model $p_{ji} = P(y_{ji} = 1)$, that is,
the probability the person $j$ answers item $i$ correctly. In other words, we
assume a Bernoulli distribution for the responses $y_{ji}$ with success
probability $p_{ji}$:
\begin{equation}
\label{bernoulli}
y_{ji} \sim \text{Bernoulli}(p_{ji})
\end{equation}
Across all models considered here, we will assume that all items measure a
single latent person trait $\theta_j$. For the present data, we can expect
$\theta_j$ to represent something closely related to fluit intelligence
[@myszkowski2018]. The most complex model we will consider in this paper is the
4-parameter logistic (4PL) model and all other simpler models result from this
model by fixing some item parameters to certain values. In the 4PL model, we
express $P(y_{ji} = 1)$ via the equation
\begin{equation}
\label{4pl}
P(y_{ji} = 1) = \gamma_i + (1 - \gamma_i - \psi_i) 
  \frac{1}{1 + \exp(-(\beta_i + \alpha_i \theta_j))}
\end{equation}
In the 4PL model, each item has 4 associated item parameters. The $\beta_i$
parameter describes the location of the item, that is, how easy or difficult it
is in general. In the above formulation of the model, higher values of $\beta_i$
imply higher success probabilities and hence $\beta_i$ can also be called the
'easiness' parameter. The $\alpha_i$ parameter describes how strongly item $i$
is related to the latent person trait $\theta_j$. We can call $\alpha_i$ 'factor
loading', 'slope', or 'discrimination' parameter, but care must be taken that
none of these terms is used uniquely and their exact meaning can only be
inferred in the context of a specific model [e.g., see @brms3 for a
somewhat different use of the term 'discrimination' in IRT models]. For our
purposes, we assume $\alpha_i$ to be positive as we expect answering the items
correctly implies higher trait scores than when answering incorrectly. The
$\gamma_i$ parameter describes the guessing proability, that is, the probability
of any person to answer item $i$ correctly even if they do not know the right
answer and thus have to guess. For obvious reasons, guessing is only relevant if
the answer space is reasonably small. In the present data, participants saw a
set of $8$ possible answers of which exactly one was considered correct. Thus,
guessing cannot be ruled out and would be equal to $\gamma_i = 1/8$ for each
item if guessing happend uniformly across all alternatives. Lastly, the
$\psi_i$ parameter describes that participants may make mistakes even if the
know the correct answer, perhaps because of inattention or simply misclicking
when selecting the chosen answer. We may call $\psi_i$ the 'lapse',
'inattention', or 'slipping' parameter. Usually these terms can be used
interchangably but, as always, the exact meaning can only be inferred in the
context of the specific model. As the answer format in the present data (i.e.,
'click on the right answer') is rather simple and participants have unlimited
time for each item, mistakes due to lapses are unlikely to appear. However, by
including a lapse parameter into our model, we are able to explictely check
whether lapses played a substantial role in the data.

We can now simplify the 4PL model in several steps
to yield the other less complex models. The 3PL model results from the 4PL model
by fixing the lapse probabilitiy to zero, that is, $\psi_i = 0$ for all
items. In the next step, we can obtain the 2PL model from the 3PL model by also
fixing the guessing proabilities to zero, that is, $\gamma_i = 0$ for all items.
In the last simplification step, we obtain the 1PL model [also known as Rasch
model; @rasch1961] from the 2PL model by assuming the factor loadings to be one,
that is, $\alpha_i = 1$ for all items. The way I described the model classes
above is top down in the way that I started with the most complex model.
However, this is not how I would recommend doing the actual model building for a
given data set, which I argue should be bottom-up, that is, starting from the
most simple (but still sensible) model. The reason is that more complex models
tend to be more complicated to fit in the sense that they both take longer
(especially when using fully Bayesian estimation) and yield more
convergence problems [e.g., @gelman2013; @brms3]. If we started by
fitting the most complex model and, after considerable waiting time, found the
model to not having converged, we may have no idea which of the several model
components were causing the problem(s). In contrast, by starting simple and
gradually building our way upwards to more complex models, we can make sure that
each model component is reasonbly specified and can be reliabily estimated
before we move further. As a result, when a problem occurs, we are likely to
have much clearer understanding of why/where the it occured and how
to fix it.

With the model likelihood fully specified by equations (\ref{bernoulli}) and 
(\ref{4pl}) (potentially with some fixed item parameters), we
are, in theory, already able to obtain estimates of person and item
parameters via maximum likelihood (ML) estimation. Howerver, there are multiple 
things that can get into our way at this point.
First, we may have simply not enough data to obtain
sensible parameter estimates. As a rule of thumb, the more complex a model
the more data we need to obtain the same estimation precision. Second,
there may be components in the model which will not be identified no matter
how much data we add. An example are binary IRT models from 2PL upwards as
(without additional structure) we cannot identify the scale of both the $\theta_j$
and $\alpha_i$ at the same time. This is because, due to the multiplictive
relationship, multiplying one of the two by a constant can be adjusted for
by dividing the other by the same constant without changing the likelihood.
Third, we need to have software that is able to do the model fitting for us,
unless we want to hand code every estimation algorithm on our own. Using existing
software requires to (re-)express our models in a way the software understands.
We will focus on the last issue first and then adress the former two. 

## IRT Models as Regression Models

There are a lot of IRT specific software packages available, in particular in
the programming language R, for example, mirt [@mirt] or TAM [@TAM; see
@brms3 for a detailed comparison]. However, in this paper, we will use the brms
package [@brms1; @brms2], which is not focussed specifically on IRT models but
more generally on (Bayesian) regression models. Accordingly, we need to
rewrite our IRT models in a form that is understandable for brms or other packages
focussed on regression models.

The first implication of this change of frameworks is that we now think of the
data in long format, with all responses from all participants on all items in
the same data column coupled with additional columns for person and item
indicators. That is, $y_{ji}$ is now formally written as $y_n$ where $n$ is the
observation number ranging from $1$ to $N = JI$. If we needed to be more explit
we could also use $y_{j_n i_n}$ to indicate that each observation number $n$ has
specific indicators $j$ and $i$ associated with it. The same goes for item and
person parameters. For example, we may write $\theta_{n_j}$ to refer to the
ability parameter of the person $j$ to whom the $n$th observation belongs.

One key aspect of regression models is that we try to express parameters on an
unconstrained space that spans the whole real line. This allows to use linear
(or more generally additive) predictor terms without having to worry about
whether these predictor terms fulfill certain boundaries, for instance, are
positive or within the unit interval $[0, 1]$. In the considered binary IRT
models, we need to ensure that the factor loadings $\alpha$ are positive and
that guessing and lapse parameters, $\gamma$ and $\psi$ respectively, are
within $[0, 1]$ as otherwise the interpreation of the latter two as
probabilities would not be sensible. In order to enforce these parameter
boundaries within a regression, we apply (inverse-)link functions. That is, for
$\alpha$ we use the log-link function (or equivalently the exponential response
function) so that
\begin{equation}
\alpha = \exp(\eta_{\alpha})
\end{equation}
where $\eta_{\alpha_n}$ is uncontrained. Similarily, for $\gamma$ and $\psi$,
we use the logit-link (or equivalently the logistic response function) so that
\begin{align}
\gamma &= \text{logistic}(\eta_{\gamma}) 
  = \frac{1}{1 + \exp(-\eta_{\gamma})}, \\
\psi &= \text{logistic}(\eta_{\psi})
  = \frac{1}{1 + \exp(-\eta_{\psi})}
\end{align}
where $\eta_{\gamma}$ and $\eta_{\psi}$ are uncontrained. The location
parameters $\beta$ are already unbounded and as such do not need an additional
link function so that simply $\beta = \eta_\beta$. The same goes for the ability
parameters $\theta$. On the scale of the linear predictors, we can perform the
usual regression operations, perhaps most importantly modeling predictor
variables or including multilevel structure. In the present data, we do not have
any additional person or item variables available so there will be no such
predictors in our models [see @brms3 for examples]. However,
there certainly is multilevel structure as we have both multiple observations
per item and per person, which we seek to model appropriately, as detailed in
the next section.

## Model Priors and Identification {#priors}

For all item parameters, we apply the non-centered parameterization of
hierarchial models [@gelman2013] as detailed in the following. We split 
the linear predictor $\eta$ (for any of the item
parameters) into an overall parameter, $\overline{b}$, and an item-specific
deviation from the overall parameter, $\tilde{b}_{i}$, so that
<!---->
\begin{equation} 
\label{ncp}
\eta_{n} = \overline{b} + \tilde{b}_{i_n} 
\end{equation} 

Without additional constraints, this split is not identified as
adding a constant to the overall parameter can be compensated by substracting
the same constant from all $\tilde{b}_{i}$ without changing the likelihood. In
Bayesian multilevel models, we approach this problem by specifying an
hierarchial prior on $\tilde{b}_{i}$ via
<!---->
\begin{equation} 
\label{bprior1}
\tilde{b}_{i} \sim \text{normal}(0, \sigma)
\end{equation}
<!---->
where $\sigma$ is the standard deviation parameter over items on the
unconstrained scale. Importantly, not only $\tilde{b}_{i}$ but also the
hyperparameters $\overline{b}$ and $\sigma$ are estimated during the model
fitting.

Using the prior distribution from (\ref{bprior1}), we would assume the
item parameters of the same item to be unrelated but, in practice,
it is very well plausible that there is actual correlation between them [@brms3].
To account for such (linear) dependency, we can extend (\ref{bprior1}) to the
multivariate case, so that we can model the vector $\tilde{\bm{b}}_i =
(\tilde{b}_{\beta_i}, \tilde{b}_{\alpha_i}, \tilde{b}_{\gamma_i},
\tilde{b}_{\psi_i})$ jointly via a multivariate normal distribution:
<!---->
\begin{equation} 
\label{bprior2}
\tilde{\bm{b}}_i \sim \text{multinormal}(0, \bm{\sigma}, \Omega)
\end{equation}
<!---->
where $\bm{\sigma} = (\sigma_{\beta}, \sigma_{\alpha}, \sigma_{\gamma}, \sigma_{\psi})$ 
is the vector of standard deviations and $\Omega$ is the correlation matrix
between of the item parameters [see also @brms1; @brms3; @nalborczyk2019].
To complete the prior specification for the item parameters, we need to 
set priors on $\overline{b}$ and $\sigma$. For this purpose, weakly-informative
normal prior on $\overline{b}$ and half-normal priors on $\sigma$ will usually
do just fine but other options are possible as well [see @brms3 for details].
By weakly-informative I mean penalizing a-priori implausible values (e.g.,
a location parameter of 1000 on the logit-scale) without affecting the
a-priori plausible parameter space too much (e.g., location parameters within
the interval $[-3, 3]$ on the logit-scale). In general, priors can only
be understood in the context of the model as a whole, which renders general
recommendation for prior specification difficult [@gelman2017].
For more details on priors for Bayesian IRT models see @brms3.

With respect to the person parameters, we specify a hierarchial prior
distribution of the same form as in (\ref{bprior1}):
<!---->
\begin{equation}
\theta_j \sim \text{normal}(0, \sigma_\theta)
\end{equation}
<!---->
where, similar as for item parameters, $\sigma_\theta$ is a standard deviation
parameter estimated as part of the model on which we put a weakly-informative 
prior. In 2PL or more complex models, we can also fix $\sigma_\theta$ to 
some value (usually $1$) as the scale is completely accounted for by
the scale of the factor loadings. However, when using weakly-informative priors
on both $\theta$ and $\alpha$ as well as on their hyperparameters, estimating
$\sigma_\theta$ actually poses no problem for model estimation.
Importantly, however, we do not include an overall person parameter
$\overline{\theta}$ as done for item parameters in (\ref{ncp}) as this would
conflict with the overall location parameter $\overline{b}_\beta$
leading to substantial convergence problems in the absense very informative
priors.

<!--
The distributions in (\ref{bprior1}) and (\ref{bprior2}) also provides the basis
for frequentist multilevel models in which it is called 'random effects
distribution' instead of a 'prior distribution'. Despite the use of different
language to describe the model assumptions, the underlying rational is quite
similar [e.g., see @gelmanMLM2006; @nalborczyk2019].
-->


# Analysis of the SPM-LS Data

```{r}
answers <- c(7, 6, 8, 2, 1, 5, 1, 6, 3, 2, 4, 5)
names(answers) <- paste0("SPM", seq_along(answers))
```

```{r}
spm_long <- read.csv("data/dataset.csv")  %>%
  mutate(person = seq_len(n())) %>%
  gather("item", "response", SPM1:SPM12) %>%
  mutate(
    answer = answers[item],
    response2 = as.numeric(response == answer),
    item = as.numeric(sub("^SPM", "", item))
  )
```

```{r}
spm_wide <- spm_long %>% 
  select(person, item, response2) %>%
  spread("item", "response2") %>%
  select(-person)
```

In this section, the Bayesian IRT models presented above will be applied to the
SPM data of @myszkowski2018. The analysed data consists of responses from 499
participants on the 12 most difficult SPM items and are freely available online
(https://data.mendeley.com/datasets/h3yhs5gy3w/1). The data gathering procedure is
described in detail in @myszkowski2018. Analyses were performed in R [@R]
using brms [@brms1] and Stan [@carpenter2017] for model specification and 
estimation. All considered models
converged well according to sample-agnostic [@vehtari2019] and sampler-specific 
[@betancourt2017] diagnostics. In the presentation of the results below, I omit
details of prior distributions and auxiliary model fitting arguments. All
details and the fully reproducible analysis are available on GitHub
(https://github.com/paul-buerkner/SPM-IRT-models). 
As discussed above, the data need to be represented in long format for
estimation in a multilevel regression framework. The relevant variables are the
binary response of the participants (variable `response2`) coded as either
correct (`1`) or incorrect (`0`) as well as `person` and `item` identifiers. In
addition to estimating the IRT models in a Bayesian way, I also estimated them
in the same way as in original analysis, that is, with the mirt package [@mirt]
using the same estimation settings as @myszkowski2018.

```{r spm-data, results="asis", eval = FALSE}
spm_long %>%
  select(response2, person, item) %>%
  mutate(
    response2 = as.integer(response2),
    person = as.integer(person),
    item = as.integer(item)
  ) %>%
  head(10) %>%
  apa_table(caption = "Glimpse of the SPM data in long format.")
```

## Model Estimation

Following the principal of building models bottom-up, we start with the 
estimation of the most simple sensible model, that is, the 1PL model.
Its formula can be specified in brms as

```{r, eval = FALSE, echo=TRUE}
formula_1pl <- bf(
  formula = response2 ~ 1 + (1 | item) + (1 | person),
  family = brmsfamily("bernoulli", link = "logit")
)
```

```{r}
fit_1pl <- brm(file = "models/fit_1pl")
```

```{r, results="hide"}
mirt_1pl <- mirt(spm_wide, model = 1, itemtype = "Rasch", SE = TRUE,
                 technical = list(NCYCLES = 10000))
```

```{r}
# extract person parameters
person_pars_1pl <- 
  ranef(fit_1pl, summary = FALSE)$person[, , "Intercept"] 

person_sds_1pl <- apply(person_pars_1pl, 1, sd)

person_pars_1pl <- person_pars_1pl %>%
  sweep(1, person_sds_1pl, "/") %>%
  posterior_summary() %>%
	as_tibble() %>%
	rownames_to_column(var = "person") %>%
  mutate(person = as.numeric(person))
```

```{r}
mirt_person_pars_1pl <- fscores(mirt_1pl, full.scores.SE = TRUE) %>%
  as.data.frame() %>%
  rename(Estimate = "F1", Est.Error = "SE_F1") %>%
  mutate(
    # person parameters are not scaled appropriately by mirt for the 1PL model
    scale = sd(Estimate) / sd(person_pars_1pl$Estimate),
    Estimate = Estimate / scale,
    Est.Error = Est.Error / scale,
    Q2.5 = Estimate - 1.96 * Est.Error, 
    Q97.5 = Estimate + 1.96 * Est.Error,
    person = seq_len(n())
  ) %>%
  select(-scale)
```

```{r}
# extract item parameters
item_pars_1pl <- coef(fit_1pl, summary = FALSE)$item[, , "Intercept"] %>%
  posterior_summary() %>%
	as_tibble() %>%
	rownames_to_column() %>%
	rename(item = "rowname") %>%
	mutate(
	  item = as.numeric(item),
	  package = "brms"
	)
```

```{r}
mirt_item_pars_1pl <- coef(mirt_1pl, as.data.frame = TRUE) %>%
  as.data.frame() %>%
  rownames_to_column() %>%
  filter(str_detect(rowname, "\\.d$")) %>%
  mutate(
    item = str_match(rowname, "^X[[:digit:]]+") %>%
      str_remove("^X") %>%
      as.numeric(),
    item = item - 0.3,
    package = "mirt"
  ) %>%
  rename(Estimate = par, Q2.5 = CI_2.5, Q97.5 = CI_97.5)
```
  
```{r item-pars-1pl, fig.height=3, fig.cap="Item parameters of the 1PL model as estimated by brms and mirt. Horizontal lines indicate 95% uncertainty intervals."}
# plot item parameters
item_pars_1pl %>%
  bind_rows(mirt_item_pars_1pl) %>%
	ggplot(aes(item, Estimate, ymin = Q2.5, ymax = Q97.5, color = package)) +
  scale_x_continuous(breaks = 1:12) +
	geom_pointrange() +
  scale_color_manual(name = "Package", values = c("black", "grey")) +
	coord_flip() +
	labs(x = "Item Number")
```

For a thorough introduction and dicsussion of the brms formula syntax
see @brms1, @brms2, and @brms3. As displayed in Figure \ref{fig:item-pars-1pl},
brms and mirt item parameter estimates of the 1PL are very similar. In addition,
their uncertainty estimates align closely as well. The brms formula for the 2PL
model looks as follows:

```{r, eval = FALSE, echo=TRUE}
formula_2pl <- bf(
  response2 ~ beta + exp(logalpha) * theta,
  nl = TRUE,
  theta ~ 0 + (1 | person),
  beta ~ 1 + (1 |i| item),
  logalpha ~ 1 + (1 |i| item),
  family = brmsfamily("bernoulli", link = "logit")
)
```

```{r}
fit_2pl <- brm(file = "models/fit_2pl")
```

```{r, results="hide"}
mirt_2pl <- mirt(spm_wide, model = 1, itemtype = "2PL", SE = TRUE,
                 technical = list(NCYCLES = 10000))
```

```{r}
# extract and scale person parameters
person_pars_2pl <- 
  ranef(fit_2pl, summary = FALSE)$person[, , "theta_Intercept"]

person_sds_2pl <- apply(person_pars_2pl, 1, sd)

person_pars_2pl <- person_pars_2pl %>%
  sweep(1, person_sds_2pl, "/") %>%
  posterior_summary() %>%
	as_tibble() %>%
	rownames_to_column(var = "person") %>%
  mutate(person = as.numeric(person))
```

```{r}
mirt_person_pars_2pl <- fscores(mirt_2pl, full.scores.SE = TRUE) %>%
  as.data.frame() %>%
  rename(Estimate = "F1", Est.Error = "SE_F1") %>%
  mutate(
    Q2.5 = Estimate - 1.96 * Est.Error, 
    Q97.5 = Estimate + 1.96 * Est.Error,
    person = seq_len(n())
  )
```

```{r}
# extract item parameters
item_pars_2pl <- coef(fit_2pl, summary = FALSE)$item

# plot item parameters
# locations
beta <- item_pars_2pl[, , "beta_Intercept"] %>%
  posterior_summary() %>%
	as_tibble() %>%
	rownames_to_column()

# slopes
alpha <- item_pars_2pl[, , "logalpha_Intercept"] %>%
  exp() %>%
  sweep(1, person_sds_2pl, "*") %>%
  posterior_summary() %>%
	as_tibble() %>%
	rownames_to_column()

item_pars_2pl <- bind_rows(beta, alpha, .id = "nlpar") %>%
	rename(item = "rowname") %>%
	mutate(item = as.numeric(item)) %>%
	mutate(
	  nlpar = factor(nlpar, labels = c("Location", "Slope")),
	  package = "brms"  
	)
```

```{r, results="hide"}
mirt_item_pars_2pl <- coef(mirt_2pl, as.data.frame = TRUE) %>%
  as.data.frame() %>%
  rownames_to_column() %>%
  filter(str_detect(rowname, "\\.(a1|d)$")) %>%
  mutate(
    item = str_match(rowname, "^X[[:digit:]]+") %>%
      str_remove("^X") %>%
      as.numeric(),
    item = item - 0.3,
    nlpar = str_match(rowname, "\\.[^\\.]+$") %>%
      str_remove("^\\.") %>%
      factor(
        levels = c("d", "a1"),
        labels = c("Location", "Slope")
      ),
    package = "mirt"
  ) %>%
  rename(Estimate = par, Q2.5 = CI_2.5, Q97.5 = CI_97.5) 
```

```{r item-pars-2pl, fig.height=3, fig.cap="Item parameters of the 2PL model as estimated by brms and mirt. Horizontal lines indicate 95% uncertainty intervals."}
# plot locations and slope next to each other
item_pars_2pl %>%
  bind_rows(mirt_item_pars_2pl) %>%
	ggplot(aes(item, Estimate, ymin = Q2.5, ymax = Q97.5, color = package)) +
  scale_x_continuous(breaks = 1:12) +
	facet_wrap("nlpar", scales = "free_x") +
  geom_pointrange() +
  scale_color_manual(name = "Package", values = c("black", "grey")) +
	coord_flip() +
	labs(x = "Item Number")
```

When comparing the formulas for the 1PL and 2PL models, we see that the
structure has changed considerably as a result of going from a generalized
linear model to a generalized non-linear model [see @brms3 for more details]. As
displayed in Figure \ref{fig:item-pars-2pl}, brms and mirt item parameter point
and uncertainty estimates of the 2PL are rather similar but not as close as for
the 1PL model. In particular, we see that the brms slope estimates of items 4
and 5 are less extreme than those of mirt presumably due to the regularization
implied by the hierarchical priors. The brms formula for the 3PL model looks as
follows:

```{r, eval = FALSE, echo=TRUE}
formula_3pl <- bf(
  response2 ~ gamma + (1 - gamma) * 
    inv_logit(beta + exp(logalpha) * theta),
  nl = TRUE,
  theta ~ 0 + (1 | person),
  beta ~ 1 + (1 |i| item),
  logalpha ~ 1 + (1 |i| item),
  logitgamma ~ 1 + (1 |i| item),
  nlf(gamma ~ inv_logit(logitgamma)),
  family = brmsfamily("bernoulli", link = "identity"),
)
```

```{r}
fit_3pl <- brm(file = "models/fit_3pl")
```

```{r, results = "hide"}
mirt_3pl <- mirt(spm_wide, model = 1, itemtype = "3PL", SE = TRUE,
                 technical = list(NCYCLES = 10000))
```

Note that, in the `family` argument, we now use `link = "identity"` instead
of `link = "logit"` and build the logit link directly into the formula via
`inv_logit(beta + exp(logalpha) * theta)`.
This is necessary to correctly include guessing parameters [@brms3]. As
displayed in Figure \ref{fig:item-pars-3pl}, brms and mirt item parameter
estimates of the 3PL model are still quite similar when it comes to locations
and slopes. However, guessing parameter estimates are quite different: mirt
obtains point estimates of 0 for all but 3 items with uncertainty intervals
ranging the whole definition space from 0 to 1. These uncertainty intervals
should probably not be taken too seriously as the guessing probabilities are
very unlikely to be anywhere close to 1 by design of the SPM test. Presumably,
this is caused by an artifact in the computation of the approximate standard
errors because point estimates are located at the boundary of the parameter
space. In contrast, point estimates of guessing parameters as obtained by brms
are close to but not exactly zero and corresponding uncertainty estimates appear
more realistic (i.e., much narrower) then those obtained by mirt.

On Github, we also report results for the 3PL model with guessing probabilities
fixed to $1/8$ derived under the assumptions that, in case of guessing, all
alternatives are equally likely. According to Figure \ref{fig:item-pars-3pl} and
model comparisons shown on GitHub, this assumption does not seem to hold for the
present data.

```{r}
# extract and scale person parameters
person_pars_3pl <- 
  ranef(fit_3pl, summary = FALSE)$person[, , "theta_Intercept"]

person_sds_3pl <- apply(person_pars_3pl, 1, sd)

person_pars_3pl <- person_pars_3pl %>%
  sweep(1, person_sds_3pl, "/") %>%
  posterior_summary() %>%
	as_tibble() %>%
	rownames_to_column(var = "person") %>%
  mutate(person = as.numeric(person))
```

```{r}
mirt_person_pars_3pl <- fscores(mirt_3pl, full.scores.SE = TRUE) %>%
  as.data.frame() %>%
  rename(Estimate = "F1", Est.Error = "SE_F1") %>%
  mutate(
    Q2.5 = Estimate - 1.96 * Est.Error, 
    Q97.5 = Estimate + 1.96 * Est.Error,
    person = seq_len(n())
  )
```

```{r}
# extract item parameters
item_pars_3pl <- coef(fit_3pl, summary = FALSE)$item

# plot item parameters
# locations
beta <- item_pars_3pl[, , "beta_Intercept"] %>%
  posterior_summary() %>%
	as_tibble() %>%
	rownames_to_column()

# slopes
alpha <- item_pars_3pl[, , "logalpha_Intercept"] %>%
	exp() %>%
  sweep(1, person_sds_3pl, "*") %>%
  posterior_summary() %>%
	as_tibble() %>%
	rownames_to_column()

# guessing parameters
gamma <- item_pars_3pl[, , "logitgamma_Intercept"] %>%
	inv_logit_scaled() %>%
  posterior_summary() %>%
	as_tibble() %>%
	rownames_to_column()

item_pars_3pl <- bind_rows(beta, alpha, gamma, .id = "nlpar") %>%
	rename(item = "rowname") %>%
	mutate(item = as.numeric(item)) %>%
	mutate(
	  nlpar = factor(nlpar, labels = c("Location", "Slope", "Guessing")),
	  package = "brms"  
	)
```

```{r, results="hide"}
mirt_item_pars_3pl <- coef(mirt_3pl, as.data.frame = TRUE) %>%
  as.data.frame() %>%
  rownames_to_column() %>%
  filter(str_detect(rowname, "\\.(a1|d|g)$")) %>%
  mutate(
    item = str_match(rowname, "^X[[:digit:]]+") %>%
      str_remove("^X") %>%
      as.numeric(),
    item = item - 0.3,
    nlpar = str_match(rowname, "\\.[^\\.]+$") %>%
      str_remove("^\\.") %>%
      factor(
        levels = c("d", "a1", "g"),
        labels = c("Location", "Slope", "Guessing")
      ),
    package = "mirt"
  ) %>%
  rename(Estimate = par, Q2.5 = CI_2.5, Q97.5 = CI_97.5) 
```

```{r item-pars-3pl, fig.height=3, fig.cap="Item parameters of the 3PL model as estimated by brms and mirt. Horizontal lines indicate 95% uncertainty intervals."}
item_pars_3pl %>%
  bind_rows(mirt_item_pars_3pl) %>%
  ggplot(aes(item, Estimate, ymin = Q2.5, ymax = Q97.5, color = package)) +
  scale_x_continuous(breaks = 1:12) +
	facet_wrap("nlpar", scales = "free_x") +
	geom_pointrange() +
  scale_color_manual(name = "Package", values = c("black", "grey")) +
	coord_flip() +
	labs(x = "Item Number")
```

```{r person-pars-3pl, fig.height=4, fig.width=8, fig.cap = "Comparison of 3PL person parameters as estimated by brms and mirt. Left: Scatter plot of point estimates. Right: Scatter plot of the associated 95% uncertainty interval widths (UIW)."}
# plot comparison of person parameters
person_pars_3pl_all <- person_pars_3pl %>% 
  bind_cols(mirt_person_pars_3pl) %>%
  mutate(
    UIW = Q97.5 - Q2.5,
    UIW1 = Q97.51 - Q2.51
  )

plot_person_pars_3pl_1 <- person_pars_3pl_all %>%
	ggplot(aes(Estimate, Estimate1, color = UIW)) +
	geom_point() +
	geom_abline() +
	scale_color_viridis_c() +
	lims(x = c(-3, 2), y = c(-3, 2)) +
	labs(
		x = "Estimate (brms)",
	  y = "Estimate (mirt)",
		color = "UIW (brms)"
	)

plot_person_pars_3pl_2 <- person_pars_3pl_all %>%
	ggplot(aes(UIW, UIW1, color = Estimate)) +
	geom_point() +
	geom_abline() +
	scale_color_viridis_c() +
	lims(x = c(1, 2.4), y = c(1, 2.4)) +
	labs(
		x = "UIW (brms)",
	  y = "UIW (mirt)",
		color = "Estimate (brms)"
	)

plot_person_pars_3pl_1 + plot_person_pars_3pl_2
```

In Figure \ref{fig:person-pars-3pl}, I display person parameter estimates of
the 3PL model. As we can see on the left-hand side of Figure
\ref{fig:person-pars-3pl}, point estimates of brms and mirt align very closely.
However, as displayed on the right-hand side of Figure
\ref{fig:person-pars-3pl}, uncertainty estimates show some deviations,
especially for more extreme point estimates (i.e., particularily good or bad
performing participants). The brms formula for the 4PL model looks as follows:

```{r, eval = FALSE, echo=TRUE}
formula_4pl <- bf(
  response2 ~ gamma + (1 - gamma - psi) * 
    inv_logit(beta + exp(logalpha) * theta),
  nl = TRUE,
  theta ~ 0 + (1 | person),
  beta ~ 1 + (1 |i| item),
  logalpha ~ 1 + (1 |i| item),
  logitgamma ~ 1 + (1 |i| item),
  nlf(gamma ~ inv_logit(logitgamma)),
  logitpsi ~ 1 + (1 |i| item),
  nlf(psi ~ inv_logit(logitpsi)),
  family = brmsfamily("bernoulli", link = "identity")
)
```

```{r}
fit_4pl <- brm(file = "models/fit_4pl")
```

```{r, results="hide"}
mirt_4pl <- mirt(
  spm_wide, model = 1, itemtype = "4PL", SE = TRUE,
  technical = list(NCYCLES = 10000)
)
```

As displayed in Figure \ref{fig:item-pars-4pl}, brms and mirt item parameter
estimates of the 4PL model differ strongly from each other. In particular, mirt
point estimates were more extreme and no uncertainty estimates could be obtained
due to singularity of the information matrix. It is plausible that the 4PL model
is too difficult to be estimated based on the given data via maximum likelihood
without further regularization.

```{r}
# extract and scale person parameters
person_pars_4pl <- 
  ranef(fit_4pl, summary = FALSE)$person[, , "theta_Intercept"]

person_sds_4pl <- apply(person_pars_4pl, 1, sd)

person_pars_4pl <- person_pars_4pl %>%
  sweep(1, person_sds_4pl, "/") %>%
  posterior_summary() %>%
	as_tibble() %>%
	rownames_to_column(var = "person") %>%
  mutate(person = as.numeric(person))
```

```{r}
mirt_person_pars_4pl <- fscores(mirt_4pl, full.scores.SE = TRUE) %>%
  as.data.frame() %>%
  rename(Estimate = "F1", Est.Error = "SE_F1") %>%
  mutate(
    Q2.5 = Estimate - 1.96 * Est.Error, 
    Q97.5 = Estimate + 1.96 * Est.Error,
    person = seq_len(n())
  )
```

```{r}
# extract item parameters
item_pars_4pl <- coef(fit_4pl, summary = FALSE)$item

# locations
beta <- item_pars_4pl[, , "beta_Intercept"] %>%
  posterior_summary() %>%
	as_tibble() %>%
	rownames_to_column()

# slopes
alpha <- item_pars_4pl[, , "logalpha_Intercept"] %>%
	exp() %>%
  sweep(1, person_sds_4pl, "*") %>%
  posterior_summary() %>%
	as_tibble() %>%
	rownames_to_column()

# guessing parameters
gamma <- item_pars_4pl[, , "logitgamma_Intercept"] %>%
	inv_logit_scaled() %>%
  posterior_summary() %>%
	as_tibble() %>%
	rownames_to_column()

# lapse parameters
psi <- item_pars_4pl[, , "logitpsi_Intercept"] %>%
	inv_logit_scaled() %>%
  posterior_summary() %>%
	as_tibble() %>%
	rownames_to_column()

item_pars_4pl <- bind_rows(beta, alpha, gamma, psi, .id = "nlpar") %>%
	rename(item = "rowname") %>%
	mutate(item = as.numeric(item)) %>%
	mutate(
	  nlpar = nlpar %>% 
	    factor(labels = c("Location", "Slope", "Guessing", "Lapse")),
	  package = "brms"
	)
```

```{r, results="hide"}
mirt_item_pars_4pl <- coef(mirt_4pl, as.data.frame = TRUE) %>%
  as.data.frame() %>%
  rownames_to_column() %>%
  filter(str_detect(rowname, "\\.(a1|d|g|u)$")) %>%
  mutate(
    item = str_match(rowname, "^X[[:digit:]]+") %>%
      str_remove("^X") %>%
      as.numeric(),
    item = item - 0.3,
    nlpar = str_match(rowname, "\\.[^\\.]+$") %>%
      str_remove("^\\.") %>%
      factor(
        levels = c("d", "a1", "g", "u"),
        labels = c("Location", "Slope", "Guessing", "Lapse")
      ),
    # invert guessing parameter
    par = ifelse(nlpar == "Lapse", 1 - par, par),
    # cannot compute CIs as inverting the information matrix fails
    CI_2.5 = par,
    CI_97.5 = par,
    package = "mirt"
  ) %>%
  rename(Estimate = par, Q2.5 = CI_2.5, Q97.5 = CI_97.5)
```

```{r item-pars-4pl, fig.height=6, fig.cap="Item parameters of the 4PL model as estimated by brms and mirt. Horizontal lines indicate 95% uncertainty intervals."}
item_pars_4pl %>%
  bind_rows(mirt_item_pars_4pl) %>%
	ggplot(aes(item, Estimate, ymin = Q2.5, ymax = Q97.5, color = package)) +
  scale_x_continuous(breaks = 1:12) +
	facet_wrap("nlpar", scales = "free_x") +
	geom_pointrange() +
  scale_color_manual(name = "Package", values = c("black", "grey")) +
	coord_flip() +
	labs(x = "Item Number")
```


## Model Comparison

Next, I investigate the required model complexity to reasonably describe the SPM
data. For this purpose, I apply Bayesian approximate leave-one-out
cross-validation [LOO-CV; @vehtari2017psis; @vehtari2017loo; @loo2018] as a
method for model comparison, which is closely related to information criteria
[@vehtari2017loo]. As shown in Table \ref{tab:loo-compare}, 3PL and 4PL models
fit substantially better than the 1PL and 2PL models, while there was little
difference between the former two. Accordingly, in the interest of parsimony, I
would tend to prefer the 3PL model if a single model needed to be chosen. This
coincides with the conclusions of @myszkowski2018. Independently of the
difference in model fit, person parameters correlated very strongly between
different models and estimation approaches, with pairwise correlations exceeding
$r = 0.97$ in all cases (see Figure \ref{fig:person-pars-all} for an
illustration).

```{r loo-compare, warning=FALSE, results= "asis"}
loo_compare <- loo_compare(fit_4pl, fit_3pl, fit_2pl, fit_1pl)

loo_data <- data.frame(
  Model = c("4PL", "3PL", "2PL", "1PL"),
  elpd = NA,
  se_elpd = NA,
  elpd_diff = NA,
  se_elpd_diff = NA
) %>%
  mutate(
    elpd = c(
      loo(fit_4pl)$estimates[1, 1],
      loo(fit_3pl)$estimates[1, 1],
      loo(fit_2pl)$estimates[1, 1],
      loo(fit_1pl)$estimates[1, 1]
    ),
    se_elpd = c(
      loo(fit_4pl)$estimates[1, 2],
      loo(fit_3pl)$estimates[1, 2],
      loo(fit_2pl)$estimates[1, 2],
      loo(fit_1pl)$estimates[1, 2]
    ),
    elpd_diff = loo_compare[, 1],
    se_elpd_diff = loo_compare[, 2]
  ) %>%
  rename(
    "ELPD" = elpd,
    "SE(ELPD)" = se_elpd,
    "ELPD-difference" = elpd_diff,
    "SE(ELPD-difference)" = se_elpd_diff
  )

loo_data %>%
  apa_table(
    caption = "Bayesian Model comparison based on the leave-one-out cross-valdidation.",
    note = "ELPD = expected log posterior density; SE = standard error. Higher ELPD values indicate better model fit. ELPD differences are in comparison to the 4PL model.",
    align = c("l", rep("r", 4)),
    format.args = list(digits = 1)
  )
```


```{r}
person_pars_all <- bind_rows(
  "1PL: brms" = person_pars_1pl,
  "1PL: mirt" = mirt_person_pars_1pl, 
  "2PL: brms" = person_pars_2pl, 
  "2PL: mirt" = mirt_person_pars_2pl, 
  "3PL: brms" = person_pars_3pl,
  "3PL: mirt" = mirt_person_pars_3pl,
  "4PL: brms" = person_pars_4pl, 
  "4PL: mirt" = mirt_person_pars_4pl, 
  .id = "model"
) %>%
  mutate(model = factor(model, levels = unique(model)))
```

```{r person-pars-all, fig.height=8, fig.width=8, fig.cap="Scatter plots, bivariate correlations, and marginal densities of person parameters from all models."}
person_pars_all %>%
  select(Estimate, model, person) %>% 
  spread(model, Estimate) %>%
  select(-person) %>%
  GGally::ggpairs(aes(alpha = 0.4))
```


# Discussion

In the present paper, I have reanalysed data to validate a short version of the
Standard Progressive Matrices [SPM-LS; @myszkowski2018] using Bayesian IRT
models via brms [@brms1; @brms2] and Stan [@carpenter2017]. By comparing
out-of-sample predictive performance, I found evidence that the 3PL model with
estimated guessing parameters outperformed simpler models and performed
similarly well than the 4PL model, which additionally estimated lapse parameters.
As specifying and fitting the 4PL model is substantially more involved than the
3PL model without apparent gains in out-of-sample predictive performance, we
argue that the 3PL model should probably be the model of choice within the scope
of all models considered here. That is, we come to a similar conclusion as
@myszkowski2018 in their original analysis despite using different frameworks
for model specification and estimation (Bayesian vs. frequentist) as well as
predictive performance [approximate leave-one-out cross-validation;
@vehtari2017loo vs. corrected AIC and $\chi^2$-based measures; @maydeu2013].

With regard to item parameters, Bayesian and frequentist estimates showed
several important differences for the most complex 3PL and 4PL IRT models.
First, point estimates of items with particularily high difficulty or slope
were more extreme in the frequentist estimation. One
central reason is likely the use weakly informative priors in the Bayesian
models which effectively shrunk extremes a little towards the mean thus
providing more conservative and robust estimates [@gelman2013]. Specifically for
the 4PL model it is likely that the model structure was also too complex to
allow for reasonable maximum likelihood estimation in the absense of any
additional regularization to stabilize inference. The latter point also
becomes apparent in the fact that the mirt package was unable to compute
standard errors of items parameters in the 4PL model due to singularity of the
information matrix. Even when computable, uncertainty estimates provided by the
frequentist IRT models were not always meaningful. For instance, in the 3PL
model, the confidence intervals of guessing parameters estimated close zero were
ranging the whole definition space between zero and one. We would argue that
this bascially represents a failure in the computational procedure of the
standard errors due the the point estimates being very close to one of the
boundaries. As such, these extreme uncertainty estimates should not be taken
seriously. In contrast, due to the use of weakly informative priors and more
involved inference procedures, which require no approximations to estimate
uncertainties, the Bayesian models provided sensible uncertainty estimates for
all item parameters of every considered IRT model.

With regard to point estimates of person parameters, we found little differences
between all considered Bayesian and frequentist IRT models. Pairwise
correlations between point estimates of two different models were all exceeding
$r = 0.97$ and often even larger than $r = 0.99$.  That is, we may consider them
as basically equivalent for all practical purposes. However, we found
substantial differences in the uncertainty estimates of person parameters across
models. That is, even though point estimates were highly similar across model
classes, it is still important to choose an appropriately complex model for the
data (i.e., the 3PL model in our case) in order to get sensible uncertainty
estimates. The latter are not only relevant for individual diagnostic purposes
but also when using person parameters as predictors in other models while taking
their estimation uncertainty into account. In addition, uncertainty estimates of
Bayesian and frequentist models varied substantially even within the same model
class, in particular for 3PL and 4PL models. Without a known ground truth, we
have no direct evidence which of the uncertainty estimates are more accurate,
but we would tend to trust the Bayesian results more due to the application of
weakly informative priors and overall more robust inference procedures for
the considered class of models.

In summary, we have were able to replicate several key findings of
@myszkowski2018. Additionally, we demonstrated that Bayesian IRT models have
some important advantages over their frequentist counterparts when it comes to
reliably fitting more complex response processes and providing sensible
uncertainty estimates for all model parameters and other quantities of interest.

# Acknowledgements

To conduct the presented analyses and create this manuscript, we used the
programming language R [@R] through the interface RStudio [@rstudio]. Further,
the following R packages were crucial (in alphabetical order): brms [@brms1],
dplyr [@dplyr], GGally [@GGally], ggplot2 [@ggplot2], kableExtra [@kableExtra],
knitr [@knitr], mirt [@mirt], papaja [@papaja], patchwork [@patchwork],
rmarkdown [@rmarkdown], rstan [@carpenter2017], and tidyr [@tidyr].

# References {-}

<div id="refs"></div>

