---
title: "Analysing Standard Progressive Matrices (SPM-LS) with Bayesian Item Response Models"
shorttitle: "Bayesian Item Response Models"
author: 
  - name: Paul-Christian Bürkner
    affiliation: "1"
    corresponding: yes
    email: paul.buerkner@gmail.com
    address: Department of Computer Science, Aalto University, Konemiehentie 2, 02150 Espoo, Finland
affiliation:
  - id: 1
    institution: Department of Computer Science, Aalto University, Finland
abstract: |
  Raven’s Standard Progressive Matrices (SPM) test and related matrix-based
  tests are widely applied measures of cognitive ability. Using Bayesian Item
  Response Theory (IRT) models, I reanalyse data of an SPM short form proposed
  by Myszkowski & Storme (2018) and, at the same time, illustrate the application 
  of these models. Results indicate that a 3-parameter logistic
  (3PL) model is sufficient to describe participants dichotomous responses
  (correct vs. incorrect) while persons' ability parameters are quite robust
  across IRT models of varying complexity. These conclusions are in line with
  the original results of Myszkowski & Storme (2018). Using Bayesian as opposed
  to frequentist IRT models offered advantages in the estimation of more complex
  (i.e., 3-4PL) IRT models and provided more sensible and robust uncertainty
  estimates.
keywords: Standard Progressive Matrics, Item Response Theory, Bayesian Statistics, brms, Stan, R
lang: english
class: doc
lineno: yes
figsintext: true
# numbersections: false
# mask: true
encoding: UTF-8
bibliography:
  - SPM_IRT_paper.bib
output:
  papaja::apa6_pdf:
    highlight: default
header-includes:
   - \usepackage{mathtools}
   - \usepackage[utf8]{inputenc}
   - \usepackage[T1]{fontenc}
   - \usepackage{textcomp}
   - \usepackage{graphicx,pdflscape}
   - \usepackage{geometry}
   - \usepackage{amsmath}
   - \usepackage{float}
   - \usepackage{supertabular}
   - \usepackage{booktabs,caption,fixltx2e}
   - \usepackage[flushleft]{threeparttable}
   - \usepackage{natbib}
   - \usepackage{tcolorbox}
   - \usepackage{paralist}
   - \usepackage{multicol}
   - \newcommand\numberthis{\addtocounter{equation}{1}\tag{\theequation}}
editor_options: 
  chunk_output_type: console
---

```{r setup, message = FALSE, warning = FALSE, results = "hide", cache = FALSE}
library(knitr)
library(kableExtra)
library(papaja)
library(tidyverse)
library(patchwork)
library(brms)
library(mirt)

# set ggplot theme
theme_set(bayesplot::theme_default())

# set rstan options
rstan::rstan_options(auto_write = TRUE)
options(mc.cores = max(1, parallel::detectCores() - 1))

# enables / disables caching for all chunks of code
knitr::opts_chunk$set(
  cache = TRUE,
  warning = FALSE,
  message = FALSE,
  echo = FALSE
)
options(knitr.kable.NA = '')

# how to use papaja ? https://crsh.github.io/papaja_man/introduction.html
```

```{r}
ll <- function(y, p) {
  y * log(p) + (1 - y) * log(1 - p)
}

fit_statistic <- function(model, criterion, group, nsamples = NULL) {
  group <- enquo(group)
  subset <- NULL
  if (!is.null(nsamples)) {
    subset <- sample(seq_len(nsamples(model)), nsamples) 
  }
  ppe <- pp_expect(model, subset = subset) %>%
    t() %>%
    as.data.frame() %>%
    cbind(model$data) %>%
    gather("draw", "ppe", starts_with("V"))
  
  yrep <- posterior_predict(model, subset = subset) %>%
    t() %>%
    as.data.frame() %>%
    cbind(spm_long) %>%
    gather("draw", "yrep", starts_with("V"))
  
  ppe %>%
    mutate(yrep = yrep$yrep) %>%
    mutate(
      crit = criterion(response2, ppe),
      crit_rep = criterion(yrep, ppe)
    ) %>%
    group_by(!!group, draw) %>%
    summarise(
      crit = sum(crit), 
      crit_rep = sum(crit_rep),
      crit_diff = crit_rep - crit
    ) %>%
    mutate(draw = as.numeric(sub("^V", "", draw))) %>%
    arrange(!!group, draw) %>%
    # group_by(person) %>%
    # summarise(bp = mean(crit_rep > crit)) %>%
    identity()
}

theme_hist <- function(...) {
  bayesplot::theme_default() +
  theme(
    axis.text.y = element_blank(),
    axis.ticks.y = element_blank(),
    axis.title.y = element_blank(),
    ...
  )
}
```

# Introduction

Raven’s Standard Progressive Matrices (SPM) test [@raven1941] and related
matrix-based tests are widely applied measures of cognitive ability [e.g.,
@jensen1988; @pind2003]. Due to their non-verbal content, which reduces biases
due to language and cultural differences, they are considered one of the purest
measures of fluid intelligence [@myszkowski2018]. However, a disadvantage of the
original SPM is that its administration takes considerable time as 60 items have
to be answered and time limits are either very loose or not imposed at all
[e.g., @pind2003]. Thus, using it as part of a bigger procedure involving
the administration of the SPM as part of a battery of tests and/or experiments
may be problematic. This is not only due to direct time restrictions but also
because participants' motivation and concentration tends to decline over the
course of the complete procedure, potentially leading to less valid measurements
[e.g., @ackerman2009].

Recently, @myszkowski2018 have proposed a short version of the original SPM
test, called SPM-LS, comprising only the last block of the 12 most complex SPM
items. They evaluated the statistical properties of the SPM-LS using methods of
Item Response Theory (IRT). IRT is widely applied in the human sciences to model
persons’ responses on a set of items measuring one or more latent constructs
[for a comprehensive introduction see @lord2012; @embretson2013;
@vanderlinden2013]. Due to its flexibility compared to Classical Test Theory
(CTT), IRT provides the formal statistical basis for most modern psychological
measurement. The best known IRT models are likely those for binary responses,
which predict the probability of a correct answer depending on item's properties
and the participant's latent abilities. As responses on SPM items can be
categorized as either right or wrong, I will focus on these binary models in
the present paper [although other models for this data are possible as well; see
@myszkowski2018]. @myszkowski2018, whose data I seek to reanalyse, used
frequenstist IRT models for inference. In this paper, I will apply Bayesian IRT
models instead and investigate potential differences to the original results. In
doing so, I hope to improve our unstanding of the robustness of the inference
obtainable from the SPM-LS test and to illustrate the application of Bayesian 
IRT methods.

# Bayesian IRT Models

In Bayesian statistics applied to IRT, we aim to estimate the posterior
distribution $p(\theta, \xi | y)$ of the person and item parameters ($\theta$
and $\xi$, respectively, which may vary in number depending on the model) given
the data $y$. We may be either interested in the posterior distribution
directly, or in quantities that can be computed on its basis. The posterior
distribution for an IRT model is defined as
\begin{equation}
p(\theta, \xi | y) = \frac{p(y | \theta, \xi) \, p(\theta, \xi)}{p(y)}.
\end{equation}
In the above equation $p(y | \theta, \xi)$ is the likelihood, $p(\theta, \xi)$
is the prior distribution and $p(y)$ is the marginal likelihood. The likelihood
$p(y | \theta, \xi)$ is the distribution of the data given the parameters and
thus relates the the data to the parameters. The prior distribution $p(\theta,
\xi)$ describes the uncertainty in the person and item parameters before having
seen the data. It thus allows to explicitely incorporate prior knowledge into
the model and/or to help identifying the model.
In practice, we will factorize the joint prior $p(\theta, \xi)$ into
the product of $p(\theta)$ and $p(\xi)$ so that we can specify priors on person
and items parameters independently. I will provide more details on likelihoods
and priors for Bayesian IRT models in the next section. The marginal likelihood
$p(y)$ serves as a normalizing constant so that the posterior is an actual
probability distribution. Except in the context of specific methods (i.e., Bayes
factors), $p(y)$ is rarely of direct interest. 

<!--
In frequentist statistics, parameter estimates are usually obtained by finding
those parameter values that maximise the likelihood. In contrast, Bayesian
statistics estimate the full (joint) posterior distribution of the parameters.
This is not only fully consistent with probability theory, but also much more
informative than a single point estimate (and an approximate measure of
uncertainty commonly known as 'standard error'). 
-->

Obtaining the posterior distribution analytically is only possible in certain
cases of carefully chosen combinations of prior and likelihood, which may
considerably limit modelling flexibilty but yield a computational advantage.
However, with the increased power of today's computers, Markov-Chain Monte-Carlo
(MCMC) sampling methods constitute a powerful and feasible alternative to
obtaining posterior distributions for complex models in which the majority of
modeling decisions is made based on theoretical and not computational grounds.
Despite all the computing power, these sampling algorithms are computationally
very intensive and thus fitting models using full Bayesian inference is usually
much slower than in point estimation techniques. If using MCMC to fit a Bayesian
model turns out to be infeasible, an alternative is to perform
optimization over the posterior distribution to obtain maximum a-posteriori
(MAP) estimates, a procedure similar to maximum likelihood estimation just with
additional regularization through priors. MCMC and MAP estimates differ
in at least two aspects. First, MCMC allows to obtain point estimates (e.g.,
means or medians) from the unidimensional marginal posteriors of the quantities
of interest, which tend to be more stable than MAP estimates obtained from the
multidimensional posterior over all parameters. Second, in contrast to MAP, MCMC
provides a set of random draws from the model parameters' posterior distribution.
After the model fitting, the posterior distribution of any quantity that is a
function of the original parameters can be obtained by applying the
function on a draw by draw basis. As such, the uncertainty in the posterior
distribution naturally propagates to new quantities, a highly desirable
property that is difficult to achieve using point estimates alone.

In the present paper, I will apply Bayesian binary IRT models to the SPM-LS data
using both MCMC and MAP estimators. Their results will be compared to those
obtained by frequentist maximum likelihood estimation. For a comprehensive
introduction to Bayesian IRT modeling see, for example, @fox2010, @levy2017, 
as well as @rupp2004.

## Bayesian IRT models for binary data

In this section, I will introduce a set of Bayesian IRT models for binary data
and unidimensional person traits. Suppose that for each person $j$ ($j = 1,
\ldots, J$) and item $i$ ($i = 1, \ldots, I$), we have observed a binary
response $y_{ji}$ which is coded as $1$ for a correct answer and $0$ otherwise.
With binary IRT models, we aim to model $p_{ji} = P(y_{ji} = 1)$, that is,
the probability the person $j$ answers item $i$ correctly. In other words, we
assume a Bernoulli distribution for the responses $y_{ji}$ with success
probability $p_{ji}$:
\begin{equation}
\label{bernoulli}
y_{ji} \sim \text{Bernoulli}(p_{ji})
\end{equation}
Across all models considered here, we will assume that all items measure a
single latent person trait $\theta_j$. For the present data, we can expect
$\theta_j$ to represent something closely related to fluit intelligence
[@myszkowski2018]. The most complex model I will consider in this paper is the
4-parameter logistic (4PL) model and all other simpler models result from this
model by fixing some item parameters to certain values. In recent years,
the 4PL model has received much attention in IRT research 
due to its flexibility in modeling complex binary response processes 
[e.g., @culpepper2016; @culpepper2017; @loken2010; @waller2017].
Under this model, we express $P(y_{ji} = 1)$ via the equation
\begin{equation}
\label{4pl}
P(y_{ji} = 1) = \gamma_i + (1 - \gamma_i - \psi_i) 
  \frac{1}{1 + \exp(-(\beta_i + \alpha_i \theta_j))}.
\end{equation}
In the 4PL model, each item has four associated item parameters. The $\beta_i$
parameter describes the location of the item, that is, how easy or difficult it
is in general. In the above formulation of the model, higher values of $\beta_i$
imply higher success probabilities and hence $\beta_i$ can also be called the
'easiness' parameter. The $\alpha_i$ parameter describes how strongly item $i$
is related to the latent person trait $\theta_j$. We can call $\alpha_i$ 'factor
loading', 'slope', or 'discrimination' parameter, but care must be taken that
none of these terms is used uniquely and their exact meaning can only be
inferred in the context of a specific model [e.g., see @brms3 for a
somewhat different use of the term 'discrimination' in IRT models]. For our
purposes, we assume $\alpha_i$ to be positive as we expect answering the items
correctly implies higher trait scores than when answering incorrectly. Also,
if we did not fix the sign of $\alpha_i$, we may run into identificaton issues
as changing the sign of $\alpha_i$ could be compensated by changing the sign
of $\theta_j$ without a change in the likelihood.

The $\gamma_i$ parameter describes the guessing proability, that is, the
probability of any person to answer item $i$ correctly even if they do not know
the right answer and thus have to guess. For obvious reasons, guessing is only
relevant if the answer space is reasonably small. In the present data,
participants saw a set of $8$ possible answers of which exactly one was
considered correct. Thus, guessing cannot be ruled out and would be equal to
$\gamma_i = 1/8$ for each item if all answer alternatives had a uniform
probability to be chosen given that a person guesses. Lastly, the $\psi_i$
parameter enables us to model the possibilty that a participant makes a mistake
even though they know the right answer, perhaps because of inattention or simply
misclicking when selecting the chosen answer. We may call $\psi_i$ the 'lapse',
'inattention', or 'slipping' parameter. Usually these terms can be used
interchangably but, as always, the exact meaning can only be inferred in the
context of the specific model. As the answer format in the present data (i.e.,
'click on the right answer') is rather simple and participants have unlimited
time for each item, mistakes due to lapses are unlikely to appear. However, by
including a lapse parameter into our model, we are able to explictely check
whether lapses played a substantial role in the answers.

We can now simplify the 4PL model in several steps to yield the other less
complex models. The 3PL model results from the 4PL model by additionally fixing
the lapse probabilitiy to zero, that is, $\psi_i = 0$ for all items. In the next
step, we can obtain the 2PL model from the 3PL model by also fixing the guessing
proabilities to zero, that is, $\gamma_i = 0$ for all items. In the last
simplification step, we obtain the 1PL model [also known as Rasch model;
@rasch1961] from the 2PL model by assuming the factor loadings to be one, that
is, $\alpha_i = 1$ for all items. Even though didactially I find it most
intuitive and helpful to introduce the models from most to least complex, I
recommend the inverse order in applications. That is, starting from the most
simple (but still sensible) model. The reason is that more complex models tend
to be more complicated to fit in the sense that they both take longer
(especially when using MCMC estimation) and yield more convergence
problems [e.g., @brms3; @gelman2013]. If we started by fitting the most complex
model and, after considerable waiting time, found the model to not have
converged, we may have no idea which of the several model components were
causing the problem(s). In contrast, by starting simple and gradually building
towards more complex models, we can make sure that each model
component is reasonbly specified and can be reliabily estimated before we move
further. As a result, when a problem occurs, we are likely to have much clearer
understanding of why/where it occured and how to fix it.

With the model likelihood fully specified by equations (\ref{bernoulli}) and 
(\ref{4pl}) (potentially with some fixed item parameters), we
are, in theory, already able to obtain estimates of person and item
parameters via maximum likelihood (ML) estimation. However, there are multiple 
potential issues that can get into our way at this point.
First, we simply may not have enough data to obtain
sensible parameter estimates. As a rule of thumb, the more complex a model,
the more data we need to obtain the same estimation precision. Second,
there may be components in the model which will not be identified no matter
how much data we add. An example are binary IRT models from 2PL upwards as
(without additional structure) we cannot identify the scale of both the $\theta_j$
and $\alpha_i$ at the same time. This is because, due to the multiplictive
relationship, multiplying one of the two by a constant can be adjusted for
by dividing the other by the same constant without changing the likelihood.
Third, we need to have software that is able to do the model fitting for us,
unless we want to hand code every estimation algorithm on our own. Using existing
software requires to (re-)express our models in a way the software understands.
I will focus on the last issue first and then adress the former two. 

## IRT Models as Regression Models

There are a lot of IRT specific software packages available, in particular in
the programming language R [@R], for example, mirt [@mirt], sirt [@sirt], or TAM
[@TAM; see @brms3 for a detailed comparison]. In addition to these more specialized
packages, general purpose probabilistic programming languages can be used to
specify and fit Bayesian IRT models, for example, BUGS [@bugs; see also
@curtis2010], JAGS [@jags; see also @depaoli2016; @zhan2019], or Stan
[@carpenter2017; see also @ames2018;@luo2018]. In this paper, I will use the
brms package [@brms1; @brms2], a higher level interface to Stan, which is not
focussed specifically on IRT models but more generally on (Bayesian) regression
models. Accordingly, we need to rewrite our IRT models in a form that is
understandable for brms or other packages focussed on regression models.

The first implication of this change of frameworks is that we now think of the
data in long format, with all responses from all participants on all items in
the same data column coupled with additional columns for person and item
indicators. That is, $y_{ji}$ is now formally written as $y_n$ where $n$ is the
observation number ranging from $1$ to $N = J \times I$. If we needed to be more explit
we could also use $y_{j_n i_n}$ to indicate that each observation number $n$ has
specific indicators $j$ and $i$ associated with it. The same goes for item and
person parameters. For example, we may write $\theta_{n_j}$ to refer to the
ability parameter of the person $j$ to whom the $n$th observation belongs.

One key aspect of regression models is that we try to express parameters on an
unconstrained space that spans the whole real line. This allows for using linear
(or more generally additive) predictor terms without having to worry about
whether these predictor terms fulfill certain boundaries, for instance, are
positive or within the unit interval $[0, 1]$. In the considered binary IRT
models, we need to ensure that the factor loadings $\alpha$ are positive and
that guessing and lapse parameters, $\gamma$ and $\psi$ respectively, are
within $[0, 1]$ as otherwise the interpreation of the latter two as
probabilities would not be sensible. In order to enforce these parameter
boundaries within a regression, we apply (inverse-)link functions. That is, for
$\alpha$ we use the log-link function (or equivalently the exponential response
function) so that
\begin{equation}
\alpha = \exp(\eta_{\alpha})
\end{equation}
where $\eta_{\alpha_n}$ is uncontrained. Similarily, for $\gamma$ and $\psi$,
we use the logit-link (or equivalently the logistic response function) so that
\begin{align}
\gamma &= \text{logistic}(\eta_{\gamma}) 
  = \frac{1}{1 + \exp(-\eta_{\gamma})}, \\
\psi &= \text{logistic}(\eta_{\psi})
  = \frac{1}{1 + \exp(-\eta_{\psi})}
\end{align}
where $\eta_{\gamma}$ and $\eta_{\psi}$ are unconstrained. The location
parameters $\beta$ are already unbounded and as such do not need an additional
link function so that simply $\beta = \eta_\beta$. The same goes for the ability
parameters $\theta$. On the scale of the linear predictors, we can perform the
usual regression operations, perhaps most importantly modeling predictor
variables or including multilevel structure. In the present data, we do not have
any additional person or item variables available so there will be no such
predictors in our models [but see @brms3 for examples if you are interested in 
this option]. However,
there certainly is multilevel structure as we have both multiple observations
per item and per person, which we seek to model appropriately, as detailed in
the next section.

## Model Priors and Identification {#priors}

When it comes to the specification of priors on item parameters, we typically
distinguish between non-hierarchical and hierarchical priors [@brms3; @fox2010;
@levy2017] with the former being applied more commonly [e.g., @fox2010;
@levy2017]. When applying non-hierarchical priors, we directly equate the linear
predictor $\eta$ (for any of the item parameter classes) with item-specific
parameters $b_i$, so that
<!---->
\begin{equation} 
\label{cp}
\eta_{n} = b_{i_n} 
\end{equation} 

for each observation $n$ and corresponding item $i$. Since $\eta$ is on an
unconstrained scale so are the $b_i$ parameters and we can apply location-scale
priors such as the normal distribution with mean $\mu$ and standard devation
$\sigma$:

\begin{equation} 
b_{i} \sim \text{normal}(\mu, \sigma)
\end{equation} 

In non-hierarchical priors, we will fix $\mu$ and $\sigma$ to sensible values.
In general, priors can only be understood in the context of the model as a
whole, which renders general recommendation for prior specification difficult
[@gelman2017]. If we only use our understanding of the scale of the modeled
parameters without any data-specific knowledge, we arrive at weakly-informative
prior distributions. By weakly-informative I mean penalizing a-priori
implausible values (e.g., a location parameter of 1000 on the logit-scale)
without affecting the a-priori plausible parameter space too much (e.g.,
location parameters within the interval $[-3, 3]$ on the logit-scale). Weakly
informative normal priors are often centered around $\mu = 0$ with
$\sigma$ appropriately chosen so that the prior covers the range of plausible
parameter values but flattens out quickly outside of that space. For more details
on priors for Bayesian IRT models see @brms3, @fox2010, as well as @levy2017.

A second class of priors for item parameters are hierarchical priors.
For this purpose, we apply the non-centered parameterization of
hierarchial models [@gelman2013] as detailed in the following. We split 
the linear predictor $\eta$ (for any of the item
parameter classes) into an overall parameter, $\overline{b}$, and an item-specific
deviation from the overall parameter, $\tilde{b}_{i}$, so that
<!---->
\begin{equation} 
\label{ncp}
\eta_{n} = \overline{b} + \tilde{b}_{i_n} 
\end{equation} 

Without additional constraints, this split is not identified as
adding a constant to the overall parameter can be compensated by substracting
the same constant from all $\tilde{b}_{i}$ without changing the likelihood. In
Bayesian multilevel models, we approach this problem by specifying an
hierarchial prior on $\tilde{b}_{i}$ via
<!---->
\begin{equation} 
\label{bprior1}
\tilde{b}_{i} \sim \text{normal}(0, \sigma)
\end{equation}
<!---->
where $\sigma$ is the standard deviation parameter over items on the
unconstrained scale. Importantly, not only $\tilde{b}_{i}$ but also the
hyperparameters $\overline{b}$ and $\sigma$ are estimated during the model
fitting.

Using the prior distribution from (\ref{bprior1}), we would assume the
item parameters of the same item to be unrelated but, in practice,
it is quite plausible that they are intercorrelated [@brms3].
To account for such (linear) dependency, we can extend (\ref{bprior1}) to the
multivariate case, so that we can model the vector $\tilde{\bm{b}}_i =
(\tilde{b}_{\beta_i}, \tilde{b}_{\alpha_i}, \tilde{b}_{\gamma_i},
\tilde{b}_{\psi_i})$ jointly via a multivariate normal distribution:
<!---->
\begin{equation} 
\label{bprior2}
\tilde{\bm{b}}_i \sim \text{multinormal}(0, \bm{\sigma}, \Omega)
\end{equation}
<!---->
where $\bm{\sigma} = (\sigma_{\beta}, \sigma_{\alpha}, \sigma_{\gamma}, \sigma_{\psi})$ 
is the vector of standard deviations and $\Omega$ is the correlation matrix
of the item parameters [see also @brms1; @brms3; @nalborczyk2019].
To complete the prior specification for the item parameters, we need to 
set priors on $\overline{b}$ and $\sigma$. For this purpose, weakly-informative
normal prior on $\overline{b}$ and half-normal priors on $\sigma$ will usually
do just fine but other options are possible as well [see @brms3 for details].

A decision between hierarchical and non-hierarchical priors is not always easy.
If in doubt, one can try out both kinds of priors and investigate whether they
make a relevant difference. Personally, I prefer hierarchical priors as they
imply some data-driven shrinkage due to their scale being learned by the model
on the fly. Also, they naturally allow item parameters to share information
across parameter classes via the correlation matrix $\Omega$.

With respect to the person parameters, it is most common to apply hierarchial
priors of the form
<!---->
\begin{equation}
\theta_j \sim \text{normal}(0, \sigma_\theta)
\end{equation}
<!---->
where, similar as for hierarchical priors on item parameters, $\sigma_\theta$ is
a standard deviation parameter estimated as part of the model on which we put a
weakly-informative prior. To give the reader in intuition: With the overall
effects in our model, we model the probability that an average person (with an
ability of zero, so imagine the ability to be centered) answers an average item
(with all item parameters at their average values which we estimate). The
varying effects then give us the deviations from the average person or item, so
that we can "customize" our prediction of the solution probability to more or
less able persons, more or less easy items, more or less discriminatory items,
and so on and so forth.

In 2PL or more complex models, we can also fix $\sigma_\theta$ to some value
(usually $1$) as the scale is completely accounted for by the scale of the
factor loadings $\sigma_\alpha$. However, when using weakly-informative priors on
both $\theta$ and $\alpha$ as well as on their hyperparameters, estimating
$\sigma_\theta$ actually poses no problem for model estimation. Importantly,
however, we do not include an overall person parameter $\overline{\theta}$ as
done for item parameters in (\ref{ncp}) as this would conflict with the overall
location parameter $\overline{b}_\beta$ leading to substantial convergence
problems in the absense very informative priors. This does not limit the model's
usefulness as only differences of person parameters are of relevance, not their
absolute values on an (in principal) arbitrary latent scale.

<!--
The distributions in (\ref{bprior1}) and (\ref{bprior2}) also provides the basis
for frequentist multilevel models in which it is called 'random effects
distribution' instead of a 'prior distribution'. Despite the use of different
language to describe the model assumptions, the underlying rational is quite
similar [e.g., see @gelmanMLM2006; @nalborczyk2019].
-->


# Analysis of the SPM-LS Data

```{r}
answers <- c(7, 6, 8, 2, 1, 5, 1, 6, 3, 2, 4, 5)
names(answers) <- paste0("SPM", seq_along(answers))
```

```{r}
spm_long <- read.csv("data/dataset.csv")  %>%
  mutate(person = seq_len(n())) %>%
  gather("item", "response", SPM1:SPM12) %>%
  mutate(
    answer = answers[item],
    response2 = as.numeric(response == answer),
    item = as.numeric(sub("^SPM", "", item))
  )
```

```{r}
spm_wide <- spm_long %>% 
  select(person, item, response2) %>%
  spread("item", "response2") %>%
  select(-person)
```

In this section, the Bayesian IRT models presented above will be applied to the
SPM data of @myszkowski2018. The analysed data consists of responses from 499
participants on the 12 most difficult SPM items and are freely available online
(https://data.mendeley.com/datasets/h3yhs5gy3w/1). The data gathering procedure is
described in detail in @myszkowski2018. Analyses were performed in R [@R] using
brms [@brms1] and Stan [@carpenter2017] for model specification and estimation
via MCMC. To investigate potential differences between hierarchical and
non-hierarchical priors on the item parameters, models were estimated for both
of these priors. Below, I will refer to these approaches as hierarchical MCMC
(MCMC-H) and non-hierarchical MCMC (MCMC-NH). Priors on person parameters were
always hierarchical and weakly informative priors were imposed on the
remaining parameters. All considered models converged well according to
sample-agnostic [@vehtari2019] and sampler-specific [@betancourt2017]
diagnostics. In the presentation of the results below, I omit details of prior
distributions and auxiliary model fitting arguments. All details and the fully
reproducible analysis are available on GitHub
(https://github.com/paul-buerkner/SPM-IRT-models). 

In addition to estimating the IRT models using MCMC, I also fitted the
models via optimization as implemented in the mirt package [@mirt].
Here, I considered two options. First, a fully frequentist approach maximizing
the likelihood under the same settings as in the original analysis of
@myszkowski2018. Second, a Bayesian optimization approach where I imposed the
same priors on item parameters as in MCMC-NH. I refer to these
two methods as maximum likelihood (ML) and maximum a-posteriori (MAP),
respectively. For models involving latent variables, such as IRT models, ML or MAP
optimization have to be combined with numerical integration over the latent
variables as the mode of the joint distribution of all parameters including
latent variables does not exist in general [e.g., see @lme4]. Such a combination
of optimization and integration is commonly referred to as
expectation-maximization (EM). A thorough discussion on EM methods is outside the
scope of the present paper but the interested reader is referred to @do2008.

```{r spm-data, results="asis", eval = FALSE}
spm_long %>%
  select(response2, person, item) %>%
  mutate(
    response2 = as.integer(response2),
    person = as.integer(person),
    item = as.integer(item)
  ) %>%
  head(10) %>%
  apa_table(caption = "Glimpse of the SPM data in long format.")
```

## Model Estimation

For estimation in a multilevel regression framework such as the one of brms, the
data need to be represented in long format. In the SPM-LS data, the relevant
variables are the binary response of the participants (variable `response2`)
coded as either correct (`1`) or incorrect (`0`) as well as `person` and `item`
identifiers. Following the principal of building models bottom-up, I start with
the estimation of the most simple sensible model, that is, the 1PL model. When
both person and item parameters are modeled hierarchically, the brms formula for
the 1PL model can be specified as

```{r, eval = FALSE, echo=TRUE}
formula_1pl <- bf(
  formula = response2 ~ 1 + (1 | item) + (1 | person),
  family = brmsfamily("bernoulli", link = "logit")
)
```

```{r item-pars-1pl, fig.height=4, fig.cap="Item parameters of the 1PL model. Horizontal lines indicate 95% uncertainty intervals."}
item_pars_1pl <- readRDS("results/item_pars_1pl")
item_pars_1pl_nh <- readRDS("results/item_pars_1pl_nh")
item_pars_1pl_map <- readRDS("results/item_pars_1pl_map")
item_pars_1pl_ml <- readRDS("results/item_pars_1pl_ml")

item_pars_1pl %>%
  bind_rows(
    item_pars_1pl_nh,
    item_pars_1pl_map, 
    item_pars_1pl_ml
  ) %>%
  mutate(
    method = factor(method, levels = c("MCMC-H", "MCMC-NH", "MAP", "ML"))
  ) %>%
	ggplot(aes(item, Estimate, ymin = Q2.5, ymax = Q97.5, color = method)) +
  scale_x_continuous(breaks = 1:12) +
	geom_pointrange() +
  scale_color_viridis_d(name = "Method") +
	coord_flip() +
	labs(x = "Item Number")
```

In order to apply non-hierarchical item parameters we have to use the formula
`response2 ~ 0 + item + (1 | person)` instead (see the code on Github for more
details). For a thorough introduction and discussion of the brms formula syntax
see @brms1, @brms2, and @brms3. As displayed in Figure \ref{fig:item-pars-1pl},
item parameter estimates of all methods are very similar for the 1PL model. In
addition, their uncertainty estimates align closely as well. The brms formula
for the 2PL model looks as follows:
  
```{r, eval = FALSE, echo=TRUE}
formula_2pl <- bf(
  response2 ~ beta + exp(logalpha) * theta,
  nl = TRUE,
  theta ~ 0 + (1 | person),
  beta ~ 1 + (1 |i| item),
  logalpha ~ 1 + (1 |i| item),
  family = brmsfamily("bernoulli", link = "logit")
)
```

```{r item-pars-2pl, fig.height=4, fig.cap="Item parameters of the 2PL model. Horizontal lines indicate 95% uncertainty intervals."}
item_pars_2pl <- readRDS("results/item_pars_2pl")
item_pars_2pl_nh <- readRDS("results/item_pars_2pl_nh")
item_pars_2pl_map <- readRDS("results/item_pars_2pl_map")
item_pars_2pl_ml <- readRDS("results/item_pars_2pl_ml")

item_pars_2pl %>%
  bind_rows(
    item_pars_2pl_nh,
    item_pars_2pl_map, 
    item_pars_2pl_ml
  ) %>%
  mutate(
    method = factor(method, levels = c("MCMC-H", "MCMC-NH", "MAP", "ML"))
  ) %>%
	ggplot(aes(item, Estimate, ymin = Q2.5, ymax = Q97.5, color = method)) +
  scale_x_continuous(breaks = 1:12) +
	geom_pointrange() +
  facet_wrap("nlpar", scales = "free_x") +
  scale_color_viridis_d(name = "Method") +
	coord_flip() +
	labs(x = "Item Number")
```

When comparing the formulas for the 1PL and 2PL models, we see that the
structure has changed considerably as a result of going from a generalized
linear model to a generalized non-linear model [see @brms3 for more details]. As
displayed in Figure \ref{fig:item-pars-2pl}, item parameter point
and uncertainty estimates of all methods are rather similar for the 2PL model
but not as close as for the 1PL model. In particular, we see that the slope estimates of items 4 and 5 vary a little presumably due to different amounts
of regularization implied by the priors. The brms formula for the 3PL model looks as
follows:

```{r, eval = FALSE, echo=TRUE}
formula_3pl <- bf(
  response2 ~ gamma + (1 - gamma) * 
    inv_logit(beta + exp(logalpha) * theta),
  nl = TRUE,
  theta ~ 0 + (1 | person),
  beta ~ 1 + (1 |i| item),
  logalpha ~ 1 + (1 |i| item),
  logitgamma ~ 1 + (1 |i| item),
  nlf(gamma ~ inv_logit(logitgamma)),
  family = brmsfamily("bernoulli", link = "identity"),
)
```

Note that, in the `family` argument, we now use `link = "identity"` instead of
`link = "logit"` and build the logit link directly into the formula via
`inv_logit(beta + exp(logalpha) * theta)`. This is necessary to correctly
include guessing parameters [@brms3]. As displayed in Figure
\ref{fig:item-pars-3pl}, item parameter estimates of all methods are still quite
similar when it comes to locations and slopes of the 3PL model. However,
guessing parameter estimates are quite different: ML obtains point estimates of
0 for all but 3 items with uncertainty intervals ranging the whole definition
space from 0 to 1. This is caused by an artifact in the computation of the
approximate standard errors because point estimates are located at the boundary
of the parameter space at which maximum likelihood theory does not hold. In
contrast, point estimates of guessing parameters as obtained by all regularized
models are close to but not exactly zero for most items and corresponding
uncertainty estimates appear more realistic (i.e., much narrower) then those
obtained by pure ML.

On Github, I also report results for the 3PL model with guessing probabilities
fixed to $1/8$ derived under the assumptions that, in case of guessing, all
alternatives are equally likely. According to Figure \ref{fig:item-pars-3pl} and
model comparisons shown on GitHub, this assumption does not seem to hold for the
present data.


```{r item-pars-3pl, fig.height=4, fig.cap="Item parameters of the 3PL model. Horizontal lines indicate 95% uncertainty intervals."}
item_pars_3pl <- readRDS("results/item_pars_3pl")
item_pars_3pl_nh <- readRDS("results/item_pars_3pl_nh")
item_pars_3pl_map <- readRDS("results/item_pars_3pl_map")
item_pars_3pl_ml <- readRDS("results/item_pars_3pl_ml")

item_pars_3pl %>%
  bind_rows(
    item_pars_3pl_nh,
    item_pars_3pl_map, 
    item_pars_3pl_ml
  ) %>%
  mutate(
    method = factor(method, levels = c("MCMC-H", "MCMC-NH", "MAP", "ML"))
  ) %>%
	ggplot(aes(item, Estimate, ymin = Q2.5, ymax = Q97.5, color = method)) +
  scale_x_continuous(breaks = 1:12) +
	geom_pointrange() +
  facet_wrap("nlpar", scales = "free_x") +
  scale_color_viridis_d(name = "Method") +
	coord_flip() +
	labs(x = "Item Number")
```

```{r person-pars-3pl, fig.height=4, fig.width=8, fig.cap = "Comparison of 3PL person parameters Left: Scatter plot of point estimates. Right: Scatter plot of the associated 95% uncertainty interval widths (UIW)."}
person_pars_3pl <- readRDS("results/person_pars_3pl")
person_pars_3pl_ml <- readRDS("results/person_pars_3pl_ml")

person_pars_3pl_all <- person_pars_3pl %>% 
  bind_cols(person_pars_3pl_ml) %>%
  mutate(
    UIW = Q97.5 - Q2.5,
    UIW1 = Q97.51 - Q2.51
  )

plot_person_pars_3pl_1 <- person_pars_3pl_all %>%
	ggplot(aes(Estimate, Estimate1, color = UIW)) +
	geom_point() +
	geom_abline() +
	scale_color_viridis_c() +
	lims(x = c(-3, 2), y = c(-3, 2)) +
	labs(
		x = "Estimate (MCMC-H)",
	  y = "Estimate (ML)",
		color = "UIW (MCMC-H)"
	)

plot_person_pars_3pl_2 <- person_pars_3pl_all %>%
	ggplot(aes(UIW, UIW1, color = Estimate)) +
	geom_point() +
	geom_abline() +
	scale_color_viridis_c() +
	lims(x = c(1, 2.4), y = c(1, 2.4)) +
	labs(
		x = "UIW (MCMC-H)",
	  y = "UIW (ML)",
		color = "Estimate (MCMC-H)"
	)

plot_person_pars_3pl_1 + plot_person_pars_3pl_2
```

In Figure \ref{fig:person-pars-3pl}, I display person parameter estimates of
the 3PL model. As we can see on the left-hand side of Figure
\ref{fig:person-pars-3pl}, ML and MCMC-H point estimates align very closely.
However, as displayed on the right-hand side of Figure
\ref{fig:person-pars-3pl}, uncertainty estimates show some deviations,
especially for more extreme point estimates (i.e., particularily good or bad
performing participants). The brms formula for the 4PL model looks as follows:

```{r, eval = FALSE, echo=TRUE}
formula_4pl <- bf(
  response2 ~ gamma + (1 - gamma - psi) * 
    inv_logit(beta + exp(logalpha) * theta),
  nl = TRUE,
  theta ~ 0 + (1 | person),
  beta ~ 1 + (1 |i| item),
  logalpha ~ 1 + (1 |i| item),
  logitgamma ~ 1 + (1 |i| item),
  nlf(gamma ~ inv_logit(logitgamma)),
  logitpsi ~ 1 + (1 |i| item),
  nlf(psi ~ inv_logit(logitpsi)),
  family = brmsfamily("bernoulli", link = "identity")
)
```

As displayed in Figure \ref{fig:item-pars-4pl}, item parameter estimates of the
4PL model differ strongly from each other for different methods. In particular,
ML point estimates were more extreme and no uncertainty estimates could be
obtained due to singularity of the information matrix. It is plausible that the
4PL model is too difficult to be estimated based on the given data via ML
without further regularization. Moreover, the estimates obtained by MCMC-H
and MCMC-NH differ noticably for some item parameters in the way that
MCMC-NH estimates tend to be more extreme and uncertain as compared to MCMC-H.
This suggests that, for these specifically chosen hierarchical and non-hierarchical
priors, the former imply stronger regularization.

```{r item-pars-4pl, fig.height=8, fig.cap="Item parameters of the 4PL model. Horizontal lines indicate 95% uncertainty intervals."}
item_pars_4pl <- readRDS("results/item_pars_4pl")
item_pars_4pl_nh <- readRDS("results/item_pars_4pl_nh")
item_pars_4pl_map <- readRDS("results/item_pars_4pl_map")
item_pars_4pl_ml <- readRDS("results/item_pars_4pl_ml")

item_pars_4pl %>%
  bind_rows(
    item_pars_4pl_nh,
    item_pars_4pl_map, 
    item_pars_4pl_ml
  ) %>%
  mutate(
    method = factor(method, levels = c("MCMC-H", "MCMC-NH", "MAP", "ML"))
  ) %>%
	ggplot(aes(item, Estimate, ymin = Q2.5, ymax = Q97.5, color = method)) +
  scale_x_continuous(breaks = 1:12) +
	geom_pointrange() +
  facet_wrap("nlpar", scales = "free_x") +
  scale_color_viridis_d(name = "Method") +
	coord_flip() +
	labs(x = "Item Number")
```


## Model Comparison

Next, I investigate the required model complexity to reasonably describe the SPM
data. For this purpose, I apply Bayesian approximate leave-one-out
cross-validation [LOO-CV; @vehtari2017psis; @vehtari2017loo; @loo2018] as a
method for model comparison, which is closely related to information criteria
[@vehtari2017loo]. I only focus on the MCMC-H models here. Results
for the MCMC-NH models are similar (see Github for details). 
As shown in Table \ref{tab:loo-compare}, 3PL and 4PL models
fit substantially better than the 1PL and 2PL models, while there was little
difference between the former two. Accordingly, in the interest of parsimony, I
would tend to prefer the 3PL model if a single model needed to be chosen. This
coincides with the conclusions of @myszkowski2018. 

```{r}
fit_1pl <- brm(file = "models/fit_1pl")
fit_2pl <- brm(file = "models/fit_2pl")
fit_3pl <- brm(file = "models/fit_3pl")
fit_4pl <- brm(file = "models/fit_4pl")
```

```{r loo-compare, warning=FALSE, results= "asis"}
loo_compare <- loo_compare(fit_4pl, fit_3pl, fit_2pl, fit_1pl)

loo_data <- data.frame(
  Model = c("4PL", "3PL", "2PL", "1PL"),
  elpd = NA,
  se_elpd = NA,
  elpd_diff = NA,
  se_elpd_diff = NA
) %>%
  mutate(
    elpd = c(
      loo(fit_4pl)$estimates[1, 1],
      loo(fit_3pl)$estimates[1, 1],
      loo(fit_2pl)$estimates[1, 1],
      loo(fit_1pl)$estimates[1, 1]
    ),
    se_elpd = c(
      loo(fit_4pl)$estimates[1, 2],
      loo(fit_3pl)$estimates[1, 2],
      loo(fit_2pl)$estimates[1, 2],
      loo(fit_1pl)$estimates[1, 2]
    ),
    elpd_diff = loo_compare[, 1],
    se_elpd_diff = loo_compare[, 2]
  ) %>%
  rename(
    "ELPD" = elpd,
    "SE(ELPD)" = se_elpd,
    "ELPD-difference" = elpd_diff,
    "SE(ELPD-difference)" = se_elpd_diff
  )

loo_data %>%
  apa_table(
    caption = "Bayesian Model comparison based on the leave-one-out cross-valdidation.",
    note = "ELPD = expected log posterior density; SE = standard error. Higher ELPD values indicate better model fit. ELPD differences are in comparison to the 4PL model.",
    align = c("l", rep("r", 4)),
    format.args = list(digits = 1)
  )
```

We can also investigate model fit using Bayesian versions of frequentist item or
person fit statistics such as log-likelihood values [@glas2003]. Independently
of which statistic $T$ is chosen, a Bayesian version of the statistic can be
constructed as follows [@glas2003]: First, the fit statistic is computed for the
observed responses $y$. We denote it by $T(y, p)$, where $p=p(\theta,\xi)$ is
the model implied response probability defined in Equation \eqref{4pl}. As $p$
depends on the model parameters, the posterior distribution over the parameters
implies a posterior distribution over $p$, which in turn implies a posterior
distribution over $T(y, p)$. Second, the fit statistic is computed for posterior
predicted responses $y_{\rm rep}$ and we denote it by $T(y_{\rm rep}, p)$. Since
$y_{\rm rep}$ reflects the (posterior distribution of) responses that would be
predicted if the model was true, $T(y_{\rm rep}, p)$ provides a natural baseline
for $T(y, p)$. Third, by comparing the posterior distributions of $T(y, p)$ and
$T(y_{\rm rep}, p)$, we can detect item- or person-specific model misfit. In
Figure \ref{fig:item-fit-1pl}, we show item-specific log-likelihood differences
between predicted and observed responses for the 1PL model. It is clearly
visible that the assumptions of the 1PL model are violated for almost half of
the items. In contrast, the corresponding results for the 3PL model look much
more reasonable (see Figure \ref{fig:item-fit-3pl}).

```{r item-fit-1pl, fig.cap="Item-specific posterior distributions of log-likelihood differences between predicted and observered responses for the 1PL model estimated via MCMC-H. If the majority of the posterior distribution is above zero, this indicates model misfit for the given item."}
item_fit_1pl <- fit_statistic(fit_1pl, criterion = ll, group = item)

item_fit_1pl %>% 
  ggplot(aes(crit_diff)) +
  geom_histogram() +
  facet_wrap("item", scales = "free") +
  theme_hist() +
  xlab("Log-likelihood difference between predicted and observed responses.")
```

```{r item-fit-3pl, fig.cap="Item-specific posterior distributions of log-likelihood differences between predicted and observered responses for the 3PL model estimated via MCMC-H. If the majority of the posterior distribution is above zero, this indicates model misfit for the given item."}
item_fit_3pl <- fit_statistic(fit_3pl, criterion = ll, group = item)

item_fit_3pl %>% 
  ggplot(aes(crit_diff)) +
  geom_histogram() +
  facet_wrap("item", scales = "free") +
  theme_hist() +
  xlab("Log-likelihood difference between predicted and observed responses.")
```

We can use the same logic to investigate person-specific model fit to find
participants for whom the models do not make good predictions. In Figure
\ref{fig:person-fit-all}, we show the predicted vs. observed log-likelihood
differences of the 192th person with response pattern $(0, 0, 1, 0, 0, 1, 0, 1,
1, 0, 0, 1)$. None of the models performs particularily well as this person
did not answer some of the easiest items correctly (i.e., items 2, 4, and 5)
but was correct on some of the most difficult items (i.e., items 8, 9, and 12).
It is unclear what was driving such a response pattern. However, one could
hypothesize that training effects over the course of the test played a role,
which are not accounted for by all models presented here. To circumvent this in
future test administrations, one could add more unevaluated practice items at
the beginning of the test so that participants have the opportunity to become
more familiar with the response format. Independently of the difference in model
fit, person parameter estimates correlated quite strongly between different
models and estimation approaches, with pairwise correlations exceeding $r =
0.97$ in all cases (see Figure \ref{fig:person-pars-all} for an illustration).

```{r}
person_fit_1pl <- 
  fit_statistic(fit_1pl, criterion = ll, group = person,
                nsamples = 2000) %>% 
  mutate(model = "1PL model")

person_fit_2pl <- 
  fit_statistic(fit_2pl, criterion = ll, group = person,
                nsamples = 2000) %>% 
  mutate(model = "2PL model")

person_fit_3pl <- 
  fit_statistic(fit_3pl, criterion = ll, group = person,
                nsamples = 2000) %>% 
  mutate(model = "3PL model")

person_fit_4pl <- 
  fit_statistic(fit_4pl, criterion = ll, group = person,
                nsamples = 2000) %>% 
  mutate(model = "4PL model")
```

```{r person-fit-all, fig.height=2, fig.cap="Person-specific posterior distributions of log-likelihood differences between predicted and observered responses for the 192th person and different models estimated via MCMC-H. If the majority of the posterior distribution is above zero, this indicates model misfit for the given person."}
person_fit_all <- bind_rows(
  person_fit_1pl, person_fit_2pl, 
  person_fit_3pl, person_fit_4pl
)

person_fit_all %>%
  filter(person == 192) %>%
  ggplot(aes(crit_diff)) +
  facet_wrap("model", nrow = 1) +
  geom_histogram() +
  theme_hist() +
  xlab("Log-likelihood difference between predicted and observed responses.")
```

```{r}
person_pars_1pl <- readRDS("results/person_pars_1pl")
person_pars_1pl_ml <- readRDS("results/person_pars_1pl_ml")
person_pars_2pl <- readRDS("results/person_pars_2pl")
person_pars_2pl_ml <- readRDS("results/person_pars_2pl_ml")
person_pars_3pl <- readRDS("results/person_pars_3pl")
person_pars_3pl_ml <- readRDS("results/person_pars_3pl_ml")
person_pars_4pl <- readRDS("results/person_pars_4pl")
person_pars_4pl_ml <- readRDS("results/person_pars_4pl_ml")

person_pars_all <- bind_rows(
  "1PL: MCMC-H" = person_pars_1pl,
  "1PL: ML" = person_pars_1pl_ml, 
  "2PL: MCMC-H" = person_pars_2pl, 
  "2PL: ML" = person_pars_2pl_ml, 
  "3PL: MCMC-H" = person_pars_3pl,
  "3PL: ML" = person_pars_3pl_ml,
  "4PL: MCMC-H" = person_pars_4pl, 
  "4PL: ML" = person_pars_4pl_ml, 
  .id = "model"
) %>%
  mutate(model = factor(model, levels = unique(model))) 
```

```{r person-pars-all, fig.height=12, fig.width=12, fig.cap="Scatter plots, bivariate correlations, and marginal densities of person parameters from MCMC-NH and ML models."}
person_pars_all %>%
  select(Estimate, model, person) %>% 
  spread(model, Estimate) %>%
  select(-person) %>%
  GGally::ggpairs(aes(alpha = 0.4)) 
```

The time required for estimation of the Bayesian models with brms via MCMC
ranged from a couple of minutes for the 1PL model to roughly half an hour for
the 4PL model (exact timings vary according to several factors, for instance,
the number of iterations and chains, applied computing machines, or the amount of
parallelization). In contrast, the corresponding optimization methods (ML and MAP)
required only a few seconds for estimation in mirt. This speed difference of multiple
orders of magnitude is typical for comparisons between MCMC and
optimization methods [e.g., @brms1]. Clearly, if speed is an issue for the
given application, full Bayesian estimation methods via MCMC should 
be applied carefully.

# Discussion

In the present paper, I have reanalysed data to validate a short version of the
Standard Progressive Matrices [SPM-LS; @myszkowski2018] using Bayesian IRT
models. By comparing out-of-sample predictive performance, I found evidence that
the 3PL model with estimated guessing parameters outperformed simpler models and
performed similarly well than the 4PL model, which additionally estimated lapse
parameters. As specifying and fitting the 4PL model is substantially more
involved than the 3PL model without apparent gains in out-of-sample predictive
performance, I argue that the 3PL model should probably be the model of choice
within the scope of all models considered here. That is, I come to a similar
conclusion as @myszkowski2018 in their original analysis despite using different
frameworks for model specification and estimation (Bayesian vs. frequentist) as
well as predictive performance [approximate leave-one-out cross-validation;
@vehtari2017loo vs. corrected AIC and $\chi^2$-based measures; @maydeu2013].

Even though I reach the same conclusions as @myszkowski2018 reached with
conventional frequentist methods, I would still like to point out some
advantages of applying Bayesian methods that we have seen in this application.
With regard to item parameters, Bayesian and frequentist estimates showed
several important differences for the most complex 3PL and 4PL IRT models.
First, point estimates of items with particularily high difficulty or slope
were more extreme in the frequentist maximum likelihood estimation. One
central reason is the use weakly informative priors in the Bayesian
models which effectively shrunk extremes a little towards the mean thus
providing more conservative and robust estimates [@gelman2013]. Specifically for
the 4PL model the model structure was also too complex to
allow for reasonable maximum likelihood estimation in the absense of any
additional regularization to stabilize inference. The latter point also becomes
apparent in the fact that no standard errors of the ML estimated items
parameters in the 4PL model could be computed due to singularity of the
information matrix. Even when formally computable, uncertainty estimates
provided by the frequentist IRT models were not always meaningful. For instance,
in the 3PL model, the confidence intervals of guessing parameters estimated
close zero were ranging the whole definition space between zero and one. This is
clearly an artifact as maximum likelihood theory does not apply at the boundary of
the parameter space and hence computation of standard errors is likely to fail.
As such, these uncertainty estimates should not be trusted. Robust alternatives
to computing approximate standard errors via maximum likelihood theory are
bootstrapping or other general purpose data resampling methods [e.g.,
@freedman1981; @mooney1993; @junker2001]. These resampling methods come with
additional computational costs as the model has to be repeatedly fitted to
different data sets but can be used even in problematic cases where standard
uncertainty estimators fail.

In contrast, due to the use of weakly informative priors the Bayesian models
provided sensible uncertainty estimates for all item parameters of every
considered IRT model. MCMC and MAP estimates provided quite similar results for
the item parameters in the context of the SPM-LS data and applied binary IRT
models. However, there is no guarantee that this will be generally the case and
so it is usually safer to apply MCMC methods when computationally feasible.
Also, for the propagation of uncertainty to new quantities, for instance,
posterior predictions, MCMC or other sampling-based methods are required. In the
case study, I have demonstrated this feature in the context of item and person
fit statistics, which revealed important insides into the assumptions of the
applied IRT models.

<!--
While priors provide a
natural and principled way to regularize inference, it should be noted that
regularization could also be implemented on top of a maximum likelihood
procedure which would have likely improved some of the more unrealistic
point estimates.
-->

With regard to point estimates of person parameters, I found little differences
between all considered Bayesian and frequentist IRT models. Pairwise
correlations between point estimates of two different models were all exceeding
$r = 0.97$ and often even larger than $r = 0.99$. This should not imply,
however, that the model choice does not matter in the context of person
parameter estimation [see also @loken2010]. Although point estimates were
highly similar, uncertainty estimates of person parameters varied substantially
across model classes. Thus, it is still important to choose an appropriately
complex model for the data (i.e., the 3PL model in our case) in order to get
sensible uncertainty estimates. The latter are not only relevant for individual
diagnostic purposes, which is undoubtly a major application of intelligence
tests, but also when using person parameters as predictors in other models while
taking their estimation uncertainty into account. In addition, uncertainty
estimates of Bayesian and frequentist models varied substantially even within
the same model class, in particular for 3PL and 4PL models. Without a known
ground truth, we have no direct evidence which of the uncertainty estimates are
more accurate (with respect to some Bayesian and/or frequentist criteria), but I
would argue in favor of the Bayesian results as they should have benefited from
to the application of weakly informative priors and overall more robust
inference procedures for the considered classes of models. All in all, it is
unsurprising that Bayesian methods have an easier time estimating uncertainty as
it is more natural to do so in a Bayesian framework. We have also seen the
important advantage of Bayesian methods that is their ability to more easily
accomodate more complex models. Yet, we have also seen that for simpler models,
Bayesian and frequentist methods provide very similar results, which really
speaks in favour of both methods and should highlight for the reader, that both
choices are valid options in this case and neither should be attacked.
Developing this understanding seems necessary with the increased application of
Bayesian methods and the accompanying arguments of whether this is a valid
option.

The analysis presented here could be extended in various directions. First,
one could fit polytomous IRT models that take into account potential differences
between distractors and thus use more information than binary IRT models. 
Such polytomous IRT models were also fitted by @myszkowski2018 and demonstrated
some information gain as compared to their binary counterparts.
Fitting these polytomous IRT models in a Bayesian framework is possible as well,
but currently not supported by brms in the here required form. Instead, one
would have to use Stan directly, or another probabilistic programming language,
whose introduction is out of scope of the present paper. Second, one
could consider multiple person traits/latent variables to investigate the
unidimensionality of the SPM-LS test. Currently, this cannot be done in brms in
an elegant manner but will be possible in the future once formal measurement
models have been implemented. For the time being, one has to fall back to full
probabilistic programming languages such as Stan or more specialized IRT
software that supports multidimensional Bayesian IRT models. According to
@myszkowski2018 the SPM-LS test is sufficiently unidimensional to justify the
application of unidimensional IRT models. Accordingly, the lack of
multidimensional models does not consitute a major limitation for the present
analysis.

In summary, I was able to replicate several key findings of
@myszkowski2018. Additionally, I demonstrated that Bayesian IRT models have
some important advantages over their frequentist counterparts when it comes to
reliably fitting more complex response processes and providing sensible
uncertainty estimates for all model parameters and other quantities of interest.

# Acknowledgements

I want to thank Marie Beisemann and two anonymous reviewers for their valuable
comments on earlier versions of the paper. To conduct the presented analyses and
create this paper, I used the programming language R [@R] through the interface
RStudio [@rstudio]. Further, the following R packages were crucial (in
alphabetical order): brms [@brms1], dplyr [@dplyr], GGally [@GGally], ggplot2
[@ggplot2], kableExtra [@kableExtra], knitr [@knitr], loo [@vehtari2017loo],
mirt [@mirt], papaja [@papaja], patchwork [@patchwork], rmarkdown [@rmarkdown],
rstan [@carpenter2017], and tidyr [@tidyr].

# References {-}

<div id="refs"></div>

